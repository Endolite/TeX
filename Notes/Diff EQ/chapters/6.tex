\documentclass[./Differential Equations.tex]{subfiles}

\begin{document}
	\section{Review of Power Series}
		\subsectionb{Power Series}
			A \textbf{power series centered at \(\bm{a}\)} is an infinite series of the form
				\[\sum_{n = 0}^\infty c_n(x - a)^n\]
		\subsectionb{Important Facts}
			\subsubsection*{Convergence}
				A power series is \textbf{convergent} at a value of \(x\) if its sequence of partial sums \(\{\{S_N(x)\}\}\) converges; that is,
					\[\lim_{N \to \infty}S_N(x) = \lim_{N \to \infty}\sum_{n = 0}^Nc_n(x - a)^n\]
					must exist. If this limit does not exist, the series is said to be \textbf{divergent}. \\
				 The \textbf{interval of convergence} is the set of \textit{all} real numbers \(x\) for which the series converges. Every power series has one. \\
				 The radius \(R\) of the interval of convergence is the \textbf{radius of convergence}. If \(R > 0\), then a power series converges for \(|x - a| < R\) (equivalently \(a - R < x < a\)  and diverges for \(|x - a| > R\). If the series is only convergent at its center, \(R = 0\). If it converges for all \(x \in \R\), then \(R = \infty\). It may or may not converge at the endpoints of the interval. \\
				 The power series \textbf{converges absolutely} within its interval of convergence (not inclusive), meaning that
				 	\[\sum_{n = 0}^\infty\left|c_n(x - a)^n\right|\]
				 	converges. \\
				 The convergence of a power series can often be determined by the \textbf{ratio test}. If \(c_n \ne 0\) for all \(n \in \N\), let
				 	\[
				 		\lim_{n \to \infty}\left|\frac{c_{n + 1}(x - a)^{n + 1}}{c_n(x - a)^n}\right|
				 			= |x - a|\lim_{n \to \infty}\left|\frac{c_{n + 1}}{c_n}\right|
				 			= L
					\]
					If \(L < 1\), the series converges absolutely. If \(L > 1\), it diverges. If \(L = 1\), the test is inconclusive. This test is always inconclusive at the endpoints of the interval of convergence.
			\subsubsection*{A Power Series Defines a Function}
				A power series defines a function
						\[f(x) = \sum_{n = 0}^\infty c_n(x - a)^n\]
						whose domain is the the series' interval of convergence. If the radius of convergence is \(R > 0\), the \(f\) is continuous, differentiable, and integrable on \(a \pm R\). If it is \(\infty\), \(f\) is continuous, differentiable, and integrable on \(\R\). \(f'(x)\) and \(\int f(x)\dd{x}\) can be found term-by-term via differentiation or integration. Convergence at the endpoints may be gained through integration or lost through differentiation. \\
					If
						\[y = \sum_{n = 0}^\infty c_nx^n\]
						is a power series, then
						\[
							y' = \sum_{n = 0}^\infty c_nnx^{n - 1} \qquad \text{and} \qquad
							y'' = \sum_{n = 0}^\infty c_nn(n - 1)x^{n - 2}
						\]
						It is then clear that the first term of \(y'\) and the first 2 of \(y''\) are 0. Omitting these, they become
						\[
							y' = \sum_{n = 1}^\infty c_nnx^{n - 1} \qquad \text{and} \qquad
							y'' = \sum_{n = 2}^\infty c_nn(n - 1)x^{n - 2}
						\]
						Note in particular the changed lower bound of the summation in the derivatives.
			\subsubsection*{Properties}
				The \textbf{identity property} states that if
					\[\sum_{n = 0}^\infty c_n(x - a)^n = 0\]
					and \(R > 0\), then \(c_n = 0\) for all \(n \in \N\). \\
				A function \(f\) is said to be \textbf{analytic at a point} if it can be represented at that point with a power series with a radius of convergence that is either positive or infinite. \\
				Power series may be combined through addition, multiplication, and division.
				\callout{17}{\paragraph{Common Maclaurin Series}
					\[\def\arraystretch{2}\begin{array}{|c|c|c|}\hline
						f(x) & \text{Maclaurin Series} & \text{Interval of Convergence} \\\hline
						e^x & \displaystyle\sum_{n = 0}^\infty \frac{1}{n!}x^n & \R \\
						\cos x & \displaystyle\sum_{n = 0}^\infty \frac{(-1)^n}{(2n)!}x^{2n} & \R \\
						\sin x & \displaystyle\sum_{n = 0}^\infty \frac{(-1)^n}{(2n + 1)!}x^{2n + 1} & \R \\
						\arctan x & \displaystyle\sum_{n = 0}^\infty \frac{(-1)^n}{2n + 1}x^{2n + 1} & [-1, 1] \\
						\cosh x & \displaystyle\sum_{n = 0}^\infty \frac{1}{(2n)!}x^{2n} & \R \\
						\sinh x & \displaystyle\sum_{n = 0}^\infty \frac{1}{(2n + 1)!}x^{2n + 1} & \R \\
						\ln(1 + x) & \displaystyle\sum_{n = 1}^\infty \frac{(-1)^{n + 1}}{n}x^n & (-1, 1] \\
						\displaystyle\frac{1}{1 - x} & \displaystyle\sum_{n = 0}^\infty x^n & (-1, 1) \\\hline
					\end{array}\]
				}
		\subsectionb{A Preview}
			To find a power series solution to DE, the desired derivatives must first be calculated. These can then be substituted back into the DE. The indices can be shifted to combine the summations. For a homogenous DE, identity can be used to solve for the coefficients.
	\section{Solutions About Ordinary Points}
		\subsectionb{A Definition}
			Dividing the homogenous linear second-order DE
			\callout{17}{\paragraph{Definition 6.2.1 Ordinary and Singular Points}
				A point \(x = x_0\) is said to be an \textbf{ordinary point} of the DE
					\[a_2(x)y'' + a_1(x)y' + a_0(x)y = 0\]
					if both \(P(x)\) and \(Q(x)\) are analytic at \(x_0\), where
					\[y'' + P(x)y' + Q(x)y = 0\]
					is the standard form of the DE. A point that is not an ordinary point of this DE is a \textbf{singular point} of it.
			}
		\subsectionb{Polynomial Coefficients}
			A polynomial is analytic at any value of \(x\), and a rational function is whenever its denominator is not zero. Both coefficients
				\[
					P(x) = \frac{a_1(x)}{a_2(x)} \qquad \text{and} \qquad
					Q(x) = \frac{a_0(x)}{a_2(x)}
				\]
				are analytic wherever \(a_2(x) \ne 0\). It then follows that \textit{a number \(x = x_0\) is an ordinary point of a linear second-order homogenous DE if \(a_2(x_0) \ne 0\), and \(x = x_0\) is a singular point if \(a_2(x_0) = 0\)}.
			\callout{17}{\paragraph{Theorem 6.2.1 Existence of Power Series Solutions}
				Let \(x = x_0\) be an ordinary point of a linear second-order homogenous DE. Two linearly independent solutions in the form of the power series
					\[y = \sum_{n = 0}^\infty c_n(x - x_0)^n\]
					can always be found. A power series solution converges at least on some interval defined by \(|x - x_0| < R\), where \(R\) is the distance from \(x_0\) to the closest singular point.
			}
			A solution of the form
				\[y = \sum_{n = 0}^\infty c_n(x - x_0)^n\]
				is said to be a \textbf{solution about the ordinary point \(\bm{x_0}\)}. The distance \(R\) is the \textit{minimum value} or \textit{lower bound} of the radius of convergence.
	\section{Solutions about Singular Points}
		\subsection{A Definition}
			A singular point \(x_0\) of the standard-form second-order homogenous linear DE
				\[y'' + P(x)y' + Q(x)y = 0\]
				can be further classified as either regular or irregular.
			\callout{17}{\paragraph{Definition 6.3.1 Regular and Irregular Singular Points}
				A singular point \(x = x_0\) is said to be \textbf{regular} if the functions
					\[
						p(x) = (x - x_0)P(x) \qquad \text{and} \qquad
						q(x) = (x - x_0)^2Q(x)
					\]
					are both analytic at \(x_0\). One that is not regular is \textbf{irregular}.
			}
		\subsectionb{Polynomial Coefficients}
			In order for \(x = x_0\) to be a singular point, either \(P(x)\) or \(Q(x)\) is not analytic at \(x_0\). As \(a_2(x)\) is a polynomial and \(x_0\) is one of its zeros, it follows that \(x - x_0\) is one of its factors. After simplifying the rational functions, then, the factor \(x - x_0\) must remain to some positive integer power in at least one of the denominators. \\
			Suppose \(x = x_0\) is a singular point and that \(p(x)\) and \(q(x)\) are analytic at \(x_0\). Multiplying \(P(x)\) by \(x - x_0\) and \(Q(x)\) by \((x - x_0)^2\) must then result in cancellation with the denominator, as \(x - x_0\) no longer appears in either. The regularity of \(x_0\) can then be determined by checking the denominators. \\
			\callout{17}{
				\textit{If \(x - x_0\) appears at most to the first power in the denominator of \(P(x)\) and at most to the second power in the denominator of \(Q(x)\), then \(x = x_0\) is a regular singular point.}
			}
			Observe also that if \(x = x_0\) is a regular singular point and the DE is multiplied by \((x - x_0)^2\), then
				\[(x - x_0)^2y'' + (x - x_0)p(x)y' + q(x)y = 0\]
				where \(p\) and \(q\) are analytic at \(x = x_0\).
		\subsectionb{Method of Frobenius}
			\callout{17}{\paragraph{Theorem 6.3.1 Frobenius' Theorem}
				If \(x = x_0\) is a regular singular point of a second-order homogenous linear DE, then there exists at least one solution of the form
					\[
						y = (x - x_0)^r\sum_{n = 0}^\infty c_n(x - x_0)^n
							= \sum_{n = 0}^\infty c_n(x - x_0)^{n + r}
					\]
					where \(r\) is a constant. The series converges at least on some interval \(0 < x - x_0 < R\).
			}
		\subsectionb{Indicial Equation}
			The \textbf{indicial equation} of a problem is the quadratic equation in \(r\) found by equating the \textit{total coefficient of the lower power of \(x\) to 0} after substituting the form of the solution into the DE. The values of \(r\) are the \textbf{indicial roots/exponents} of the singularity \(x = x_0\). These values can then be substituted into a recurrence relation, relating \(c_k\) to \(c_{k + 1}\). Frobenius' theorem guarantees that at least one solution of the assumed series form can be found. \\
			The indicial equation can be obtained without substituting. If \(x = 0\) is a regular singular point, then both 
				\[
					p(x) = xP(x) \qquad \text{and} \qquad
					q(x) = x^2Q(x)
				\]
				are analytic at \(x = 0\); that is, their power series expansions 
					\[
						p(x) = \sum_{n = 0}^\infty a_nx^n \qquad \text{and} \qquad
						q(x) = \sum_{n = 0}^\infty b_nx^n
					\]
				are valid on intervals with positive radii of convergence. Multiplying the standard form by \(x^2\) yields
				\[x^2y'' + x[xP(x)]y' + [x^2Q(x)]y = 0\]
				Substituting
				\[y = \sum_{n = 0}^\infty c_nx^{n + r}\]
				and the series expansions of \(p(x)\) and \(q(x)\) into this yields the general indicial equation
				\[r(r - 1) + a_0r + b_0 = 0\]
				where \(a_0\) and \(b_0\) are the constant terms of the power series expansions of \(p(x)\) and \(q(x)\) respectively.
		\subsectionb{Three Cases}
			Let \(x = 0\) be a regular singular point of a linear second-order homogenous DE and \(r_1\) and \(r_2\) be real. When employing the method of Frobenius, three cases can be distinguished that correspond to the nature of the indicial roots.
			\subsubsectionb{Case I}
				If \(r_1 > r_2\) and \(r_1 - r_2\) is not a positive integer, then there exist 2 linearly independent solutions of the DE of the form
					\[
						y_1(x) = \sum_{n = 0}^\infty c_nx^{n + r_1}, \quad c_0 \ne 0 \qquad \text{and} \qquad
						y_2(x) = \sum_{n = 0}^\infty b_nx^{n + r_2}, \quad b_0 \ne 0
					\]
			\subsubsectionb{Case II}
				If \(r_1 - r_2 = N\) where \(N \in \Z^+\), then there exist 2 linearly independent solutions of the form
					\[
						y_1(x) = \sum_{n = 0}^\infty c_nx^{n + r_1}, \quad c_0 \ne 0 \qquad \text{and} \qquad
						y_2(x) = Cy_1(x)\ln x + \sum_{n = 0}^\infty b_nx^{n + r_2}, \quad b_0 \ne 0
					\]
					where \(C\) is a constant (that may be 0).
			\subsubsectionb{Case III}
				If \(r_1 = r_2\), then there exist 2 linearly independent solutions of the form
					\[
						y_1(x) = \sum_{n = 0}^\infty c_nx^{n + r_1}, \quad c_0 \ne 0 \qquad \text{and} \qquad
						y_2(x) = y_1(x)\ln x + \sum_{n = 1}^\infty b_nx^{n + r_1}
					\]
		\subsectionb{Finding a Second Solution}
			In case II, there may be 2 solutions of the form
				\[y = \sum_{n = 0}^\infty c_nx^{n + r}\]
				This is cannot be known in advance, instead being determined after finding the indicial roots and examining the recurrence relation that defines the coefficients. It is possible that \(C = 0\); that is,
				\[
					y_1(x) = \sum_{n = 0}^\infty c_nx^{n + r_1} \qquad \text{and} \qquad
					y_2(x) = \sum_{n = 0}^\infty b_nx^{n + r_2}
				\]
	\section{Special Functions}
		\subsectionb{Solution of Bessel's Equation}
			\textbf{Bessel's equation of order \(\bm{\nu}\)} is the DE
				\[x^2y'' + xy' + \left(x^2 - \nu^2\right)y = 0\]
				As \(x = 0\) is a singular point of this equation, there exists at least one solution of the form
				\[
					y = \sum_{n = 0}^\infty c_nx^{n + r} \implies
						y' = \sum_{n = 0}^\infty (n + r)c_nx^{n + r - 1} \implies
						y'' = \sum_{n = 0}^\infty (n + r)(n + r - 1)c_nx^{n + r - 2}
				\]
				Substituting this into the DE,
				\begin{align*}
					0 &= \sum_{n = 0}^\infty\left[(n + r)(n + r - 1)c_nx^{n + r}\right] + \sum_{n = 0}^\infty\left[(n + r)c_nx^{n + r}\right] + \sum_{n = 0}^\infty\left[\left(x^2 - \nu^2\right)c_nx^{n + r}\right] \\
						&= x^r\sum_{n = 1}^\infty\left[\left((n + r)^2 - \nu^2\right)c_nx^n\right] + x^r\sum_{n = 0}^\infty\left[c_nx^{n + 2}\right] + c_0\left(r(r - 1) + r - \nu^2\right)x^r \\
						&= x^r\sum_{n = 1}^\infty\left[\left((n + r)^2 - \nu^2\right)c_nx^n\right] + x^r\sum_{n = 0}^\infty\left[c_nx^{n + 2}\right] + c_0\left(r^2 - \nu^2\right)x^r
				\end{align*}
				Rewriting the equation in standard form,
				The indicial equation is then
				\[0 = r^2 - \nu^2\]
				which has roots \(r = \pm\nu\), so
				\[
					y_1 = \sum_{n = 0}^\infty c_nx^{n + \nu} \qquad \text{and} \qquad
					y_2 = \sum_{n = 0}^\infty c_nx^{n - \nu}
				\]
				Substituting \(y_1\) into the DE,
				\begin{align*}
					0 &= x^\nu\sum_{n = 1}^\infty\left[\left(n^2 + 2n\nu\right)c_nx^n\right] + x^\nu\sum_{n = 0}^\infty\left[c_nx^{n + 2}\right] \\
						&= x^\nu\left((1 + 2\nu) + \underset{k = n - 2}{\underbrace{\sum_{n = 2}^\infty\left[c_n(n + 2\nu)x^n\right]}} + \underset{k = n}{\underbrace{\sum_{n = 0}^\infty\left[c_nx^{n + 2}\right]}}\right) \\
						&= x^\nu\left((1 + 2\nu)c_1x + \sum_{k = 0}^\infty\left[(k + 2)(k + 2 + 2\nu)c_{k + 2} + c_k\right]x^{k + 2}\right)
				\end{align*}
				The recurrence relation is then
				\[c_{k + 2} = -\frac{c_k}{(k + 2)(k + 2 + 2\nu)}\]
				Letting \(c_1 = 0\) implies that \(c_3 = c_5 = \cdots = 0\), so for \(k = 0, 2, \cdots\), letting \(k + 2 = 2n\) for \(n \in \Z^+\) means that
				\[c_{2n} = -\frac{c_{2n - 2}}{2^2n(n + \nu)}\]
				This can be rewritten in terms of \(c_0\) as
				\[c_{2n} = \frac{(-1)^nc_0}{2^{2n}n!\prod\limits_{i = 1}^n\left[i + \nu\right]}\]
				It is standard practice to let \(c_0\) be the specific value
				\[c_0 = \frac{1}{2^\nu\Gamma(1 + \nu)}\]
				where \(\Gamma(1 + \nu)\) is the gamma function. This posses the property that
				\[\Gamma(1 + \alpha) = \alpha\Gamma(\alpha)\]
				so the denominator of \(c_{2n}\) can be further reduced to a single term:
				\[c_{2n}= \frac{(-1)^n}{2^{2n + \nu}n!\Gamma(1 + \nu + n)}\]
		\subsectionb{Bessel Functions of the First Kind}
			Using the coefficients \(c_{2n}\) and \(r = \nu\), a series solution of Bessel's equation is
				\[y = \sum_{n = 0}^\infty c_{2n}x^{2n + \nu}\]
				This solution is typically denoted by \(J_\nu(v)\):
				\[J_\nu(x) = \sum_{n = 0}^\infty \frac{(-1)^n}{n!\Gamma(1 + \nu + n)}\left(\frac{x}{2}\right)^{2n + \nu}\]
				If \(\nu \ge 0\), the series converges at least on \([0, \infty)\). For the second root \(r_2 = -nu\),
				\[J_{-\nu}(x) = \sum_{n = 0}^\infty \frac{(-1)^n}{n!\Gamma(1 - \nu + n)}\left(\frac{x}{2}\right)^{2n - \nu}\]
				The above two functions are called \textbf{Bessel functions of the first kind} of orders \(\nu\) and \(-\nu\) respectively. The value of \(\nu\) may result in negative power of \(x\), making the interval of convergence \((0, \infty)\). \\
				When \(\nu\) = 0, \(J_\nu = J_{-\nu}\). \\
				If \(\nu > 0\) and \(r_1 - r_2 = 2\nu\) is not a positive integer, then it follows from Case I that \(J_\nu\) and \(J_{-\nu}\) are linearly independent solutions on \((0, \infty)\), making the general solution on the interval
				\[y = C_1J_\nu(x) + C_2J_{-\nu}(x)\]
				When \(2\nu\) is a positive integer, then from Case II it follows that a second series solution \textit{may} exist. If \(\nu = m \in \Z^+\), \(J_m\) and \(J_{-m}\) are not linearly independent. Additionally, \(2\nu\) can be a positive integer when \(\nu\) is half of an odd positive integer. In this case, \(J_\nu\) and \(J_{-\nu}\) are linearly independent. In summary, The general solution of the Bessel equation on \((0, \infty)\) is
				\[y = C_1J_\nu(x) + C_2J_{-\nu}(x), \quad \nu \notin \Z\]
		\subsectionb{Bessel Functions of the Second Kind}
			If \(v \notin \Z\), the function defined by the linear combination
				\[Y_\nu(x) = \frac{\cos(\nu\pi)J_\nu(x) - J_{-\nu}(x)}{\sin(\nu\pi)}\]
				and the function \(J_\nu(x)\) are linearly independent solutions of Bessel's equation. Another form for the general solution of this equation is then
				\[y = C_1J_\nu(x) + C_2Y_\nu(x), \quad \nu \notin \Z\]
				As \(\nu \to m \in \Z\), \(Y_\nu(x)\) has the indeterminate form \(0/0\). It can be shown by L'H\^opital's Rule that this limit exists. Moreover, the function
				\[Y_m = \lim_{\nu \to m}Y_\nu(x)\]
				and \(J_m(x)\) are linearly independent solutions of Bessel's equation. For \textit{any} value of \(\nu\), then, the general solution of Bessel's equation on \((0, \infty)\) can be written as
				\[y = C_1J_\nu(x) + C_2Y_\nu(x)\]
				\(Y_\nu(x)\) is called the \textbf{Bessel function of the second kind of order \(\bm{\nu}\)}.
		\subsectionb{DEs Solvable in Terms of Bessel Functions}
			Some DEs can be transformed via change change of variable into Bessel's equation. Letting \(t = \alpha x\) where \(\alpha > 0\), for example, yields the \textbf{parametric Bessel equation of order \(\bm{\nu}\)}:
				\[x^2y'' + xy' + \left(\alpha^2x^2 - \nu^2\right)y = 0\]
				then by chain rule
				\[
					\dv{y}{x} = \dv{y}{t}{t}{x} 
						= \alpha\dv{y}{t}
				\]
				and
				\[
					\dv[2]{y}{x} = \dv{t}\left(\dv{y}{x}\right)\dv{t}{x} 
						= \alpha^2\dv[2]{y}{t}
				\]
				Substituting these back in,
				\begin{align*}
					0 &= \left(\frac{t}{\alpha}\right)^2\alpha^2\dv[2]{y}{t} + \left(\frac{t}{\alpha}\right)\alpha\dv{y}{t} + \left(t^2 - \nu^2\right)y \\
						&= t^2\dv[2]{y}{t} + t\dv{y}{t} + \left(t^2 - \nu^2\right)y
				\end{align*}
				This is Bessel's equation of order \(\nu\) with solution
				\[y = C_1J_\nu(t) + C_2Y_\nu(t)\]
				Resubstituting \(t = \alpha x\) yields
				\[y = C_1J_\nu(\alpha x) + C_2Y_\nu(\alpha x)\]
\end{document}
