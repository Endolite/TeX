\documentclass[./Differential Equations]{subfiles}

\begin{document}
	\section{Preliminary Theory---Linear Equations}	
		\subsection{Initial-Value and Boundary-Value Problems}
			\subsubsectionb{Initial-Value Problem}
				For a linear DE, an \textbf{\(\bm{n^{\th}}\)-order initial-value problem (IVP)} is
				\begin{align*}
					a_n(x)\dv[n]{y}{x} + a_{n - 1}\dv[n - 1]{y}{x} + \cdots + a_1(x)\dv{y}{x} + a_0y &= g(x) \\ 
					\text{subject to} \quad y(x_0) &= y_0, y'(x_0) = y_1, \ldots, y^{(n - 1)}(x_0) = y_{n - 1}
				\end{align*}
			\subsubsectionb{Existence and Uniqueness}
				\callout{17}{\paragraph{Theorem 4.1.1 Existence of a Unique Solution}
					Let \(a_n(x)\), \(a_{n - 1}(x)\), \(\ldots\), \(a_1(x)\), \(a_0(x)\), and \(g(x)\) be continuous on an interval \(I\) and let \(a_n(x) \ne 0\) for every \(x\) in the interval. If \(x = x_0\) at any point within \(I\), then a solution \(y(x)\) of the IVP both exists on the interval and is unique.
				}
			\subsubsectionb{Boundary-Value Problem}
				A linear DE of order two or greater in which the dependent variable or its derivatives are specified at \textit{different points}, such as
				\[a_2(x)\dv[2]{y}{x} + a_1(x)\dv{y}{x} + a_0(x)y = g(q) \qquad \text{subject to} \qquad y(a) = y_0, y(b) = y_1\]
				is called a \textbf{boundary-value problem (BVP)}. The specified values are called \textbf{boundary conditions (BC)}. A solution of the above problem is a function that satisfies the DE on some interval \(I\) containing both \(a\) and \(b\) that pases through the points \(a, y_0\) and \(b, y_2\).
		\subsection{Homogenous Equations}
			A linear \(n^{\th}\)-order DE of the form
				\[a_n(x)\dv[n]{y}{x} + a_{n - 1}(x)\dv[n - 1]{y}{x} + \cdots + a_1(x)\dv{y}{x} + a_0(x)y = 0\]
				is said to be \textbf{homogenous} while one of the form
				\[a_n(x)\dv[n]{y}{x} + a_{n - 1}(x)\dv[n - 1]{y}{x} + \cdots + a_1(x)\dv{y}{x} + a_0(x)y = g(x)\]
				where \(g(x)\) is not identically 0 is said to be \textbf{nonhomogenous}. \\
			It should be noted that word \textit{homogenous} as used here does not refer to coefficients that are homogenous functions.
			\callout{17}{
				When stating definitions or theorems regarding linear equations, it shall always be assumed that on some common interval \(I\), the coefficient functions \(a_i(x)\) and \(g(x)\) are continuous and that \(a_n(x) \ne 0\) for every \(x\) in the interval.
			}
			\subsubsectionb{Differential Operators}
				The symbol \(D\) is called a \textbf{differential operator}, as it transforms a differentiable function into another function. In general,
					\[\dv[n]{y}{x} = D^ny\]
					where \(y\) is a sufficiently differentiable function. Polynomial expressions that involve \(D\) are also differential operators. In general, an \textbf{\(\bm{n^{\th}}\)-order differential/polynomial operator} to be
					\[L = a_n(x)D^n + a_{n - 1}(x)D^{n - 1} + \cdots + a_1(x)D + a_0(x)\]
				As \(D(c(f(x)) = cDf(x)\) (where \(c\) is a constant) and \(D\{f(x) + g(x)\} = Df(x) + Dg(x)\), the differential operator \(L\) also posses the property that acting on a linear combination of differentiable functions is the same as the linear combination of \(L\) operating on the individual functions. Symbolically,
					\[L\{\alpha f(x) + \beta g(x)\} = \alpha Lf(x) + \beta Lg(x)\]
					where \(\alpha\) and \(\beta\) are constant. Because of this property, \(L\) can be said to be a \textbf{linear operator}.
			\subsubsectionb{Differential Equations}
				Any linear DE can be expressed in terms of \(D\) notation.
			\subsubsectionb{Superposition Principle}
				\callout{17}{\paragraph{Theorem 4.1.2 Superposition Principle---Homogenous Equations}
					Let \(y_1, y_2, \ldots, y_k\) be solutions of the homogenous \(n^{\th}\)-order DE \(L(y) = 0\) on an interval \(I\). The linear combination
						\[y = c_1y_1(x) + c_2y_2(x) + \cdots + c_ky_k(x)\]
						where the \(c_i\) are arbitrary constants, is also a solution of the DE on \(I\).
				}
			\subsubsectionb{Linear Dependence and Linear Independence}
				\callout{17}{\paragraph{Linear Dependence/Independence}
					A set of functions \(f_1(x), f_x(x), \ldots, f_n(x)\) is said to be \textbf{linearly dependent} on an interval \(I\) if there exist constants \(c_i\) (that are not all 0) such that
						\[c_1f_1(x) + c_2f_2(x) + \cdots + c_nf_n(x) = 0\]
						for every \(x\) in the interval. If a set of functions are not linearly dependent on an interval, they are \textbf{linearly independent}.
				}
				\textit{If a set of two functions are linearly dependent, then they are constant multiples of each other.} \\
				A set of functions is linearly dependent on a interval if at least one of the functions can be expressed as a linear combination of the others.
			\subsubsectionb{Solutions of Differential Equations}
				\callout{17}{\paragraph{Wronskian}
					Let each of the functions \(f_1(x), f_2(x), \ldots, f_n(x)\) possess at least \(n - 1\) derivatives. The determinant
						\[
							W(f_1, f_2, \ldots, f_n) = \begin{vmatrix}f_1 & f_2 & \cdots & f_n \\ f_1' & f_2' & \cdots & f_n' \\ \vdots & \vdots && \vdots \\ f_1^{(n - 1)} & f_2^{(n - 1)} & \cdots & f_n^{(n - 1)}\end{vmatrix}
						\]
						is the \textbf{Wronskian} of the functions.
				}
				\callout{17}{\paragraph{Theorem 4.1.3 Criterion for Linearly Independent Solutions}
					Let \(y_1, y_2, \ldots, y_n\) be \(n\) solutions of the homogenous linear \(n^{\th}\)-order DE \(L(y) = 0\) on interval \(I\). The set of solutions is \textbf{linearly independent} on \(I\) if an only if \(W(y_1, y_2, \ldots, y_n) \not\equiv 0\) for every \(x\) in the interval.
				}
				\callout{17}{\paragraph{Fundamental Set of Solutions}
					Any set of \(n\) linearly independent solutions of the homogenous linear \(n^{\th}\)-order linear DE \(L(y) = 0\) on an interval \(I\) is said to be a \textbf{fundamental set of solutions} on the interval.
				}
				\callout{17}{\paragraph{Theorem 4.1.4 Existence of a Fundamental Set}
					A fundamental set of solutions for the homogenous linear \(n^{\th}\)-order DE \(L(y) = 0\) exists on any interval \(I\).
				}
				\callout{17}{\paragraph{General Solution---Homogenous Equations}
					Let \(y_1, y_2, \ldots, y_n\) be a fundamental set of solutions of the homogenous linear \(n^{\th}\)-order DE \(L(y) = 0\) on an interval \(I\). The \textbf{general solution} of the equation on the interval is given by
						\[y = c_1y_1(x) + c_2y_2(x) + \cdots + c_ny_n(x)\]
						where \(c_i\) are arbitrary constants.
				}
		\subsection{Nonhomogenous Equations}
			Any function \(y_p\) with no parameters that satisfies a nonhomogenous equation is said to be a \textbf{particular solution}. If \(y_{1 \cdots k}\) corresponds to solutions of a homogenous equation on interval \(I\) and \(y_p\) is a particular solution of a nonhomgenous equation on the same interval, the linear combination
				\[y = c_1y_1(x) + c_2y_2(x) + \cdots + c_ky(k) + y_p(x)\]
			\callout{17}{\paragraph{Theorem 4.1.6 General Solution---Nonhomogenous Equation}
				Let \(y_p\) be any particular solution of the nonhomogenous linear \(n^{\th}\)-order DE \(L(y) = g(x)\) on interval \(I\) and let \(y_{1 \cdots}\) be a fundamental set of solutions of the corresponding homogenous DE \(L(y) = 0\) on the same interval. The \textbf{general solution} of the equation on the interval is
					\[y = c_1y_1(x) + c_2y_2(x) + \cdots + c_ny_n(x) + y_p(x)\]
					where \(c_{i \cdots n}\) are arbitrary constants.
			}
			\subsubsectionb{Complementary Functions}
				The general solution of a nonhomegenous DE can be rewritten as the sum of two functions
					\[y = c_1y_1(x) + c_2y_2(x) + \cdots + c_ny_n(x) + y_p(x) = y_c(x) + y_p(x)\]
					\(y_c(x)\), which is a general solution of the corresponding homogenous DE, is called the \textbf{complementary function} of the nonhomogenous DE. The general solution of a nonhomgenous DE can then be rewritten as
					\[y = \text{complementary function} + \text{any particular solution} = y_c + y_p\]
			\subsubsectionb{Another Superposition Principle}
				\callout{17}{\paragraph{Superposition Principle -- Nonhomogenous Equations}
					Let \(y_{p, i \cdots k}\) be \(k\) particular solutions to a nonhomogenous linear \(n^{\th}\)-order DE of the form \(L(y) - g(x)\) on an interval \(I\) that correspond to \(k\) distinct functions \(g_{1 \cdots k}\); that is, suppose \(y_p, i\) denotes a particular solution to the DE
						\[a_n(x)y^{(n)} + a_{n - 1}(x)y^{(n - 1)} + \cdots + a_1(x)y' + a_0(x)y = g_i(x)\]
						Then
						\[y_p(x) = y_{p, 1}(x) + y_{p, 2}(x) + \cdots + y_{i, k}(x)\]
						is a particular solution of
						\[a_n(x)y^{(n)} + a_{n - 1}(x)y^{(n - 1)} + \cdots + a_1(x)y' + a_0(x)y = g_1(x) + g_2(x) + \cdots + g_k(x)\]
				}
	\section{Reduction of Order}
		\subsectionb{Reduction of Order}
			Suppose that \(y_1\) denotes a nontrivial solution of the second-order homogenous linear DE
				\[a_2(x)y''+ a_1(x)y' + a_0(x)y = 0\]
				that is defined on \(I\). If a second solution \(y_2\) is linearly independent on \(I\), then their quotient \(y_2/y_1\) is nonconstant on \(I\). Knowing this, a function \(u(x)\) can be found by substituting \(y_2(x) = u(x)y_1(x)\) into the DE. This method is called \textbf{reduction of order}, as a linear first-order DE must be solved to find \(u\).
		\subsectionb{General Case}
			Putting the second-order homogenous linear DE into \textbf{standard form},
				\[y'' + P(x)y' + Q(x)y = 0\]
				where \(P(x)\) and \(Q(x)\) are continuous on \(I\). Let \(y_1\) be a known solution to this DE on \(I\) that is never equal to 0 on the interval. Defining \(y = u(x)y_1(x)\),
				\begin{align*}
					y' &= u'y_1 + y_1'u \\
					y'' &= u''y_1 + y_1'u' + y_1''u + u'y_1'
							= uy_1'' + 2u'y_1' + u''y_1 \\
					y'' + Py' + Qy &= u(y_1'' + Py_1' + Qy_1) + y_1u'' + (2y' + Py_1)u' \\
						0 &= y_1u'' + (2y_1' + Py_1)u'
				\end{align*}
				Letting \(w = u'\),
				\[0 = y_1w' + (2y' + Py_1)w\]
				This equation is both linear and separable.
				\begin{align*}
					\frac{\dd{w}}{w} + 2\frac{y_1'}{y_1}\dd{x} + P\dd{x} &= 0 \\
					\ln|wy_1^2| &= -\int P \dd{x} \\
					wy_1^2 &= C\en^{-\int P \dd{x}}
				\end{align*}
				Substituting \(u\) back in,
				\[u = C\int \frac{\en^{-\int P \dd{x}}}{y_1^2} \dd{x}\]
				Letting \(C = 1\) and knowing that \(y(x) = u(x)y_1(x)\),
				\[y_2(x) = y_1(x)\int \frac{\en^{-\int P(x) \dd{x}}}{y_1^2(x)} \dd{x}\]
				In the case that this integral is nonelementary, \(y_2(x)\) can be defined as an integral function:
				\[y_2(x) = y_1(x)\int_{x_0}^x \frac{\en^{-\int P(t) \dd{t}}}{y_1^2(t)} \dd{t}\]
	\section{Homogenous Linear Equations with Constant Coefficients}
		Substituting \(y = \en^{mx}\) into the first-order homogenous DE
			\[ay' + by = 0\]
			yields
			\[am\en^{mx} + b\en^{mx} = \en^{mx}(am + b) = 0\]
			As \(\en^{x} = 0\) has no real solutions, this is only satisfied when \(am + b = 0\). For this value of \(m\), \(y = \en^{mx}\) is a solution of the DE.
		\subsectionb{Auxiliary Equation}
			Consider the second-order equation
				\[ay'' + by' + cy = 0\]
				where \(a\), \(b\), and \(c\) are constants. Substituting \(y = \en^{mx}\), this becomes
				\[am^2\en^{mx} + bm\en^{mx} + c\en^{mx} = \en^{mx}(am^2 + bm + c) = 0\]
				\(\en^{x} = 0\) has no real solutions, 
				\[am^2 + bm + c = 0\]
				This is called the \textbf{auxiliary equation} of the DE. Using the quadratic formula to solve for \(m\),
				\[m = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}\]
				The discriminant \(b^2 - 4ac\) can be used to determine the number of real solutions:
				\[\begin{array}{|c|ccc|}\hline
					b^2 - 4ac & > 0 & 0 & < 0 \\\hline
					\text{real solutions} & 2 & 1 & 1 \\\hline
				\end{array}\]
			\subsectionb{Case I: Distinct Real Roots}
				If the auxiliary equation has two distinct real roots \(m_1\) and \(m_2\), two solutions \(y_1 = \en^{m_1x}\) and \(y_2 = \en^{m_2x}\) are found. These two functions are linearly independent on \(\R\), making the general solution on the interval
					\[y = C_1\en^{m_1x} + C_2\en^{m_2x}\]
			\subsectionb{Case II: Repeated Real Roots}
				When only a single real root \(m_1\) of the auxiliary equation exists, only a single exponential solution \(y_1 = \en^{m_1x}\) is found. As \(m_1 = m_2\) only if \(b^2 - 4ac = 0\), \(m_1 = -b/2a\). It then follows from reduction of order that a second solution is
					\[
						y_2 = \en^{m_1x}\int \frac{\en^{-\int -\frac{b}{a} \dd{x}}}{\en^{2mx}} \dd{x}
								= \en^{mx}\int \frac{\en^{\int 2m \dd{x}}}{\en^{2mx}} \dd{x}
								= \en^{mx}\int \frac{\en^{2mx}}{\en^{2mx}} \dd{x}
								= \en^{mx}\int \dd{x}
								= x\en^{mx}
					\]
					The general solution is then
					\[y = C_1\en^{mx} + C_2x\en^{mx}\]
			\subsectionb{Case III: Complex Conjugate Roots}
				If \(m_1\) and \(m_2\) are both complex, it can be written that
					\[
						m_1 = \alpha + i\beta \qquad \text{and} \qquad
								m_2 = \alpha - i\beta
					\]
					where \(\alpha, \beta \in \R^+\) (\(\R^+\) being the set of positive real numbers). \\
					This is formally identical to having two distinct real roots, so the general solution can therefore be written as
					\[y = C_1\en^{(\alpha + i\beta)x} + C_2\en^{(\alpha - i\beta)x}\]
					In practice, though, real functions are preferred to complex exponentials. As such, \textbf{Euler's formula}
					\[\en^{i\theta} = \cos\theta + i\sin\theta\]
					(where \(\theta\) is any real number) is used. It then follows that
					\[
						\en^{i\beta x} = \cos\beta x + i\sin\beta x \qquad \text{and} \qquad
								\en^{-i\beta x} = \cos\beta x - i\sin\beta x
					\]
					The addition and subtraction of these two formulas results in
					\[
						\en^{i\beta x} + \en^{-i\beta x} = 2\cos\beta x \qquad \text{and} \qquad
								\en^{i\beta x} - \en^{-i\beta x} = 2i\sin \beta x	
					\]
					Letting \(C_1 = C_2 = 1\) results in
					\[
						y_1 = \en^{(\alpha + i\beta)x} + \en^{(\alpha - i\beta)x}
					\]
					while letting \(C_1 = 1\) and \(C_2 = -1\) gives
					\[
						y_2 = \en^{(\alpha + i\beta)x} - \en^{(\alpha - i\beta)x}
					\]
					Refactoring,
					\[
						y_1 = \en^{\alpha x}\left(\en^{i\beta x} + \en^{-i\beta x}\right) = 2\en^{\alpha x}\cos\beta x \qquad \text{and} \qquad
								y_2 = \en^{\alpha x}\left(\en^{i\beta x} - \en^{-\beta x}\right) = 2i\en^{\alpha x}\sin\beta x
					\]
					As \(2\) and \(i\) are constants, \(\en^{\alpha x}\cos\beta x\) and \(\en^{\alpha x}\sin\beta x\) are \textit{real} solutions. These two solutions form a fundamental set on \(\R\). The general solution is consequently
					\[
						y = C_1\en^{\alpha x}\cos\beta x + C_2\en^{\alpha x}\sin\beta x
							= \en^{\alpha x}(C_1\cos\beta x + C_2\sin\beta x) 
					\]
			\subsectionb{Two Equations Worth Knowing}
				The DEs
					\[y'' \pm k^2y = 0\]
					where \(k \in \R^+\) are quite useful in applied mathematics. \\
				The auxiliary equation of \(y'' + k^2y = 0\) is
					\[m^2 + k^2 = 0\]
					which has imaginary roots \(m_1 = ki\) and \(m_2 = -ki\). With \(\alpha = 0\) and \(\beta = k\), the general solution can be seen to be
					\[y = C_1\cos kx + C_2\sin kx\]
				The auxiliary equation of \(y'' - k^2y = 0\) is
					\[m^2 - k^2 = 0\]
					which has real roots \(m_1 = k\) and \(m_2 = -k\), making the general solution of the DE
					\[y = C_1\en^{kx} + C_2\en^{kx}\]
				Letting \(C_1 = C_2 = 1/2\), and \(C_1 = 1/2\) and \(C_2 = -1/2\) yields the particular solutions
					\[
						y_1 = \frac{1}{2}\left(\en^{kx} + \en^{-kx}\right) = \cosh kx \qquad \text{and} \qquad
								y_2 = \frac{1}{2}\left(\en^{kx} - \en^{-kx}\right) = \sinh kx 
					\]
					meaning that another form of the general solution is
					\[y = C_1\cosh(kx) + C_2\sinh(kx)\]
			\subsectionb{Higher-Order Equations}
				In general, in order to solve an \(n^{\th}\)-order homogenous DE \(L(y) = 0\), the \(n^{\th}\) degree polynomial
					\[a_nm^n + a_{n - 1}m^{n - 1} + \cdots + a_2m^2 + a_1m + a_0 = 0\]
					must be solved. If all roots of this equation are distinct, the general solution is
					\[y = C_1\en^{m_1x} + C_2\en^{m_2} + \cdots + C_n\en^{m_nx}\]
	\section{Undetermined Coefficients---Superposition Approach}
		To solve a nonhomogenous DE \(L(y) = g(x)\), the complementary function \(y_c\) and \textit{any} particular solution must be found. The general solution is then \(y = y_c + y_p\). \\
		The complementary function \(y_c\) is the general solution of the associated homogenous DE \(L(y) = 0\).
		\subsectionb{Method of Undetermined Coefficients}
			The \textbf{method of undetermined coefficients} is a technique of obtaining a particular solution \(y_p\) of a nonhomogenous DE. This method is limited to linear DEs with constant coefficients and \(g(x)\) comprised of finite products and sums of constants, polynomials, exponentials, or sines or cosines. The reason for this limitation is that these types of functions have the property that the derivatives of their sums or products are comprised of sums or products of themselves. Because the linear combination of derivatives \(y_p\) must be identical to \(g(x)\) it is reasonable to assume that \(y_p\) must also \textit{have the same form} as \(g(x)\).
			\callout{17}{\paragraph{Trial Particular Solutions}
				\[\def\arraystretch{1.3}\begin{array}{|c|c|}\hline
					g(x) & \text{Form of } y_p \\\hline
					k & A \\
					ax^n + bx^{n - 1} + \cdots & Ax^n + Bx^{n - 1} + \cdots \\
					\sin(\alpha x) & \multirow{2}*{\(A\cos(\alpha x) + B\sin(\alpha x)\)} \\
					\cos(\alpha x) & \\
					\en^{kx} & A\en^{kx} \\\hline
				\end{array}\]
				The form of \(y_p\) when \(g(x)\) is the product of two functions is that of one function with the second function's substituted in for the constants. If \(g(x) = (ax + b)\sin x\), for example, then \(y_p = (Ax + B)\cos x + (Cx + E)\sin x\). When \(g(x)\) is a sum, the form is simply the sum of the forms of the individual terms of \(g(x)\).
			}
		\subsectionb{Case I}
			\callout{17}{\paragraph{Form Rule for Case I}
				\textit{The form of \(y_p\) is a linear combination of all linearly independent functions generated by repeated differentiations of \(g(x)\).}
			}
		\subsectionb{Case II}
			\callout{17}{\paragraph{Multiplication Rule for Case II}
				\textit{If any \(y_{p,i}\) contains duplicate terms from \(y_c\), then that \(y_{p,i}\)} must be multiplied by \(x^n\) where \(n\) is the smallest positive integer that eliminates the duplication.
			}
	\section{Undetermined Coefficients---Annihilator Approach}
		An \(n^{\th}\)-order DE can be written
			\[a_nD^ny + a_{ n- 1}D^{n - 1}y + \cdots + a_1Dy + a_0y = g(x)\]
			where \(D^k = \dv*[k]{y}{x}\) for \(k = 0, \ldots n\). This can also be written \(L(y) = g(x)\), where \(L\) denotes the \(n^{\th}\)-order differential operator
			\[L(y) = a_nD^n + a_{n - 1}D^{n - 1} + \cdots + a_1D + a_0\]
			This operator notation is not only useful as shorthand, but also serves to justify the rules for determining the form of \(y_p\).
		\subsectionb{Factoring Operators}
			When the coefficients \(a_i\) are real constants, a linear differential operator \(L\) can be factored if the characteristic polynomial
				\[a_nm^n + a_{n - 1}m^{n - 1} + \cdots + a_1m + a_0\]
				can be factored; that is to say, if \(r_1\) is a root of the auxiliary equation
				\[a_nm^n + a_{n - 1}m^{n - 1} + \cdots + a_1m + a_0 = 0\]
				then \(L = (D - r_1)P(D)\), the polynomial expression \(P(D)\) being a linear differential operator of order \(n - 1\).
			\callout{13.19}{
				\textit{Factors of a linear differential operator with constant coefficients commute.}
			}
		\subsectionb{Annihilator Operator}
			If \(L\) is a linear differential operator with constant coefficients and \(f\) is a sufficiently differential function such that \(L(f(x)) = 0\), then \(L\) is said to be an \textbf{annihilator} of \(f\). \\
			The functions annihilated by \(L\) are simply those that can be obtained from the general solution of the homogenous DE \(L(y) = 0\). \\
			A polynomial can be annihilated by an operator that annihilates the highest power \(x\). \\
			The differential operator \((D - \alpha)^n\) annihilates any \(x^k\en^{\alpha x}\) for \(k\) in \(0, \ldots, n - 1\). \\
			The differential opeartor \((D^2 - 2\alpha D + \alpha^2 + \beta^2)^n\) annihilates \(x^k\en^{\alpha x}\cos\beta x\) and \(x^k\en^{\alpha x}\sin\beta x\) for \(k\) in \(0, \ldots, n - 1\). \\
			It should be noted that multiple differential operators may be able to annihilate a function, so when finding one, that of the \textit{lowest possible order} should be sought.
		\subsectionb{Undetermined Coefficients}
			Let \(L(y) = g(x)\) be a linear DE with constant coefficients such that \(g(x)\) meets the condition for undetermined coefficients to be used, being that it is a linear combination of functions of the forms
				\[
					k, \quad
						x^m, \quad
						x^m\en^{\alpha x},  \quad
						x^m\en^{\alpha x}\cos\beta x, \quad \text{and} \quad
						x^m\en^{\alpha x}\sin\beta x
				\]
				where \(m \in \N\) and \(\alpha, \beta \in \R\). Such a function can be annihilated by a differential operator \(L_1\) of lowest order, which consists of a product of
				\[
					D^n, \quad
					(D - \alpha)^n \quad \text{and} \quad
					(D - 2\alpha D + \alpha^2 + \beta^2)^n
				\]
				Applying \(L_1\) to both sides of the DE yields
				\[L_1L(y) = L_1(g(x)) = 0\]
				By solving the \textit{homogenous higher-order} equation \(L_1L(y) = 0\), the \textit{form} of \(y_p\) can be found. Substituting this into \(L(y) = g(x)\), an explicit particular solution can be found. This procedure is called the \textbf{method of undetermined coefficients}.
		\subsectionb{Summary of the Method}
			\callout{17}{\paragraph{Undetermined Coefficients---Annihilator Approach}
				The DE \(L(y) = g(x)\) has constant coefficients and \(g(x)\) is comprised of the finite sums and products of constants, polynomials, exponentials, sines, and cosines.
				\begin{enumerate}
					\item
						Find the complementary solution \(y_c\) to the homogenous equation \(L(y) = 0\).
					\item
						Operate on both sides of the nonhomogenous equation \(L(y) = g(x)\) using differential operator \(L_1\) that annihilates \(g(x)\).
					\item
						Find the general solution of the higher-order homogenous DE \(L_1L(y) = 0\).
					\item
						Delete all terms that are duplicated in \(y_c\). Form a linear combination of those that remain. This is the form of a particular solution of \(L(y) = g(x)\).
					\item
						Substitute \(y_p\) into \(L(y) = g(x)\). Match coefficients of the various functions on each side of the equality and solve the resulting system of equations for the unknown coefficients of \(y_p\).
					\item
						Form the general solution \(y = y_c + y_p\).
				\end{enumerate}
			}
	\section{Variation of Parameters}
		\subsectionb{Linear First-Order DEs Revisited}
			The general solution of the linear first-order DE
				\[a_1(x)y' + a_0(x)y = g(x)\]
				is found by first writing it in standard form as
				\[\dv{y}{x} + P(x)y = f(x)\]
				If \(P(x)\) and \(f(x)\) are continuous on common interval \(I\), the integrating factor can be used to identify the solution as
				\[y = C\en^{-\int P(x) \dd{x}} + \en^{-\int P(x) \dd{x}}\int \en^{-\int P(x) \dd{x}}f(x) \dd{x}\]
				This is of the form \(y_c + y_p\), where
				\[
					y_p = C\en^{-\int P(x)\dd{x}} \qquad \text{and} \qquad
						y_c = \en^{-\int P(x)\dd{x}}\int\en^{-\int P(x) \dd{x}}f(x) \dd{x}
				\]
				This particular solution can be derived via \textbf{variation of parameters}. \\
			Suppose that \(y_1\) is a particular solution of a homogenous first-order linear DE; that is,
				\[\dv{y_1}{x} + P(x)y_1 = 0\]
				The solution
				\[y = \en^{-\int P(x) \dd{x}}\]
				is also known, and as the equation is linear, \(C_1y_1(x)\) is its general solution. Variation of parameters consists of finding a particular solution of				
				\[\dv{y}{x} + P(x)y = 0\]
				that is of the form
				\[y_p = u_1(x)g_1(x)\]
				The \textit{parameter} \(C_1\) has been replaced by the \textit{function} \(u_1\). \\
				Substituting \(y_p = u_1g_1\) into the equation,
				\begin{align*}
					f(x) &= \dv{y}{x}[u_1y_1] + P(x)u_1y_1 \\
						&= u_1\dv{y_1}{x} + y_1\dv{u_1}{x} + P(x)u_1y_1 \\
						&= u_1\overset{0}{\overbrace{\left(\dv{y_1}{x} + P(x)y_1\right)}} + y_1\dv{u_1}{x} \\
						&= y_1\dv{u_1}{x}
				\end{align*}
				Separating variables and integrating,
				\begin{align*}
					\dd{u_1} &= \frac{f(x)}{y_1(x)} \dd{x} \\
					u_1 &= \int \frac{f(x)}{y_1(x)} \dd{x}
				\end{align*}
				The particular solution is therefore
				\[y = u_1y_1 = y_1\int \frac{f(x)}{y_1(x)} \dd{x}\]
		\subsectionb{Linear Second-Order DEs}
			Consider the linear second-order DE
				\[a_2(x)y'' + a_1(x)y' + a_0y = g(x)\]
				In standard form,
				\[y'' + P(x)y' + Q(x)y = f(x)\]
				Suppose that \(P(x)\), \(Q(x)\), and \(f(x)\) are continuous on common interval \(I\). The complementary solution 
				\[y_c = C_1y(x) + C_2y(x)\]
				Replacing the parameters with functions,
				\[y_p(x) = u_1(x)y_1(x) + u_2(x)y_2(x)\]
				Using product rule,
				\begin{align*}
					y_p' &= u_1'y_1 + y_1'u_1 + u_2'y_2 + y_2'u_2 \\
					y_p'' &= u_1''y_1 + y_1'u_1' + y_1''u_1 + u_1'y_1' + u_2''y_2 + y_2'u_2 + y_2''u_2 + u_2'y_2'
				\end{align*}
				Substituting,
				\begin{align*}
					y_p'' + P(x)y_p' + Q(x)y_p &= u_1 \overset{0}{\overbrace{(y_1'' + Py_1' + Qy_1)}} + u_2(y) + u_2\overset{0}{\overbrace{(y_2'' + Py_2' + Qy_2)}} \\
						&\quad + y_1u_1'' + u_1'y_1' + y_2u_2'' + u_2'y_2' + P(y_1u_1' + y_2u_2') + y_1'u_1' + y_2'u_2'\\
						&= \dv{}{x}[y_1u_1'] + \dv{}{x}[y_2u_2'] + P(y_1u_1' + y_2u_2') + y_1'u_1' + y_2'u_2' \\
						&= \dv{}{x}[y_1u_1' + y_2u_2'] + P(y_1u_1' + y_2u_2') + y_1u_1' + y_2u_2'
								= f(x)
				\end{align*}
				As there are two unknowns, two equations are needed. These are obtained by further assuming that
				\[y_1u_1' + y_2u_2' = 0\]
				as this results in DE reducing to
				\[y_1u_1 + y_2u_2 = f(x)\]
				This provides the desired two equations. By Cramer's Rule, the solution of this system can be expressed in terms of determinants:
				\[
					u_1' = \frac{W_1}{W} = -\frac{y_2f(x)}{W} \qquad \text{and} \qquad
						u_2' = \frac{W_2}{W} = \frac{y_1f(x)}{W}
				\]
				where
				\[
					W = \begin{vmatrix}
 							y_1 & y_2 \\
 							y_1' & y_2'
 						\end{vmatrix}, \quad
 						W_1 = \begin{vmatrix}
 									0 & y_2 \\
 									f(x) & y_2'
 								\end{vmatrix}, \quad \text{and} \quad
						W_2 = \begin{vmatrix}
 									y_1 & 0 \\
 									y_1' & f(x)
 								\end{vmatrix}
				\]
				The functions \(u_1\) and \(u_2\) can then be found simply by integrating the results. \\
				The determinant \(W\) is the Wronskian of \(y_1\) and \(y_2\). As they are linearly independent, \(W(y_1(x), y_2(x))\) is never 0 for any \(x\) in \(I\).
		\subsectionb{Summary of the Method}
			Due to the long-winded nature of the procedure, it is more efficient here to simply memorize the formulas for first- and second-order equations. \\
			Solving
				\[a_2y'' + a_1y' + a_0y = g(x)\]
				the complementary function can be found as
				\[y_c = C_1y_1 + C_2y_2\]
				The Wronskian \((W(y_1(x), y_2(x))\) can then be computed. Dividing by \(a_1\), the DE can be put into standard form
				\[y'' + P(x)y' + Q(x)y = f(x)\]	
				to determine \(f(x)\). \(u_1\) and \(u_2\) can be as
				\[
					u_1 = -\int \frac{y_1f(x)}{W} \dd{x} \qquad \text{and} \qquad
						u_2 = \int \frac{y_1f(x)}{W} \dd{x}
				\]
				A particular solution is then
				\[y_p = u_1y_1 + u_2y_2\]
				The general solution is then
				\[
					y = y_c + y_p
						= C_1y_2 + C_2y_2 + u_1y_2 + u_2y_2
				\]
		\subsectionb{Higher-Order Equations}
			The method used for second-order DEs can be generalized to those of order \(n\) in standard form
				\[y^{(n)} + P_{n - 1}(x)y^{(n - 1)} + \cdots + P_1(x)y' + P_0(x)y = f(x)\]
				If
				\[y_c = C_1y_1 + C_2y_2 + \cdots + C_ny_n\]
				then a particular solution is
				\[y_p = u_1y_1 + u_2y_2 + \cdots + u_ny_n\]
				where \(u_k'\) is determined by the equations
				\[
					\begin{array}{ccccccccc}
						y_1u_1' &+ &y_2u_2' &+ &\cdots &+ & y_nu_n' &= &0 \\
						y_1'u_1' &+ & y_2'u_2' &+ &\cdots &+ & y_n'u_n' &= &0 \\
						\vdots &&&&&&&& \vdots \\
						y_1^{(n - 1)}u_1 &+ & y_2^{(n - 1)}u_2' &+ &\cdots &+ & y_n^{(n - 1)}u_n' &= &0
					\end{array}
				\]
				The first \(n - 1\) equations in this system are assumptions made to simplify the resultant equation of substituting \(y_p\) into the DE. Cramer's Rule gives
				\[u_k' = \frac{W_k}{W}\]
				where \(W\) is the Wronskian of \(y_{1 \cdots n}\) and \(W_k\) is the determinant obtained by replacing the \(k^{\th}\) column of the Wronskian with a column comprised of \((0, 0, \ldots, f(x))\).
	\section{Cauchy-Euler Equations}
		\subsectionb{Cauchy-Euler Equation}
			A linear DE of the form
				\[a_nx^n\dv[n]{y}{x} + a_{n - 1}x^{n - 1}\dv[n - 1]{y}{x} + \cdots + a_1x\dv{y}{x} + a_0y = g(x)\]
				(where the coefficients \(a_{0 \cdots n}\) are constants) is a \textbf{Cauchy-Euler equation}. The defining characteristic of this type of equation is the fact that the degree of the monomial coefficients \(x^k\) match the orders \(k\) of differentiation. \\
			It should be noted that the lead coefficient \(a_nx^n\) of any Cauchy-Euler equatino is zero at \(x = 0\), so to ensure the existence of a unique solution, the interval \((0, \infty)\) is generally considered.
		\subsectionb{Method of Solution}
			Substituting \(x^m\) into a Cauchy-Euler equation yields a polynomial in \(m\) times \(x^m\), as
				\[
					a_kx^k\dv[k]{y}{x} 
						= a_kx^km(m - 1)(m - 2)\cdots(m - k + 1)x^{m - k} 
						= a_km(m - 1)(m - 2)\cdots(m - k + 1)x^m
				\]
				Substituting into a second-order equation,
				\[
					ax^2\dv[2]{y}{x} + bx\dv{y}{x} + cy
						= am(m - 1)x^m + bmx^m + cx^m
						= (am(m - 1) + bm + c)x^m
				\]
				Thus \(y = x^m\) is a solution of the DE if \(m\) is a solution of the \textbf{auxiliary equation}
					\[
						am(m - 1) + bm + c = 0 \qquad \text{or} \qquad
						am^2 + (b - a)m + c = 0
					\]
		\subsectionb{Case I: Distinct Real Roots}
			Let \(m_1\) and \(m_2\) denote distinct real roots of the auxiliary equation. This implies that \(y_1 = x^{m_1}\) and \(y_2 = x^{m_2}\) form a fundamental set of solutions, making the general solution
				\[y = C_1x^{m_1} + C_2x^{m_2}\]
		\subsectionb{Case II: Repeated Real Roots}
			If the roots of the auxiliary equation are repeated, then only a single solution \(y = x^m\) is obtained. The roots of a quadratic being zero necessitates that the discriminant be 0, meaning that
				\[m = -\frac{b - a}{2a}\]
				Rewriting the Cauchy-Euler Equation in standard form yields
				\[\dv[2]{y}{x} + \frac{b}{ax}\dv{y}{x} + \frac{c}{ax^2}y = 0\]
				meaning that
				\[
					P(x) = \frac{b}{ax} \qquad \text{and} \qquad
					\int P(x) \dd{x} = \frac{b}{a}\ln{x}
				\]
				Thus
				\begin{align*}
					y_2 &= x^m\int \frac{\en^{-\frac{b}{a}\ln x}}{x^{2m}} \dd{x} 
							= x^m\int\left[x^{-b/a} \times x^{-2m}\right]\dd{x} 
							= x^m\int\left[x^{-b/a} \times x^{\frac{b - a}{a}}\right]\dd{x} \\
						&= x^m\int \frac{\dd{x}}{x}
								= x^m\ln{x}
				\end{align*}
				The general solution is therefore
				\[y = C_1x^m + C_2x^m\ln{x}\]
		\subsectionb{Case III: Conjugate Complex Roots}
			If the roots of the auxiliary equation are the conjugate pair \(m = \alpha \pm i\beta\) (where \(\alpha\) and \(\beta > 0\) are real, then a solution is
				\[y = C_1x^{\alpha + i\beta} + C_2x^{\alpha + i\beta}\]
				The positive complex term can be rewritten as
				\[
					x^{i\beta} = \en^{i\beta\ln{x}}
						= \cos(\beta\ln{x}) + i\sin(\beta\ln{x})
				\]
				The negative complex term can similarly be rewritten as
				\[x^{-i\beta} = \cos(\beta\ln{x}) - i\sin(\beta\ln{x})\]
				Adding and subtracting these results yield
				\[
					x^{i\beta} + x^{-i\beta} = 2\cos(\beta\ln{x}) \qquad \text{and} \qquad
					x^{i\beta} - x^{-i\beta} = 2i\sin(\beta\ln{x})
				\]
				Letting \(C_1 = C_2 = 1\) gives
				\[
					y_1 = x^\alpha\left(x^{i\beta} + x^{-i\beta}\right)
						= 2x^\alpha\cos(\beta\ln{x})
				\]
				while letting \(C_1 = 1\) and \(C_1 = -1\) yields 
				\[
					y_2 = x^\alpha\left(x^{i\beta} - x^{-i\beta}\right)
						= 2ix^\alpha\sin(\beta\ln{x})
				\]
				As 
				\[W\left(x^\alpha\cos(\beta\ln{x}), x^\alpha\sin(\beta\ln{x})\right) = \beta x^{2n - 1} \not\equiv 0\]
				for \(\beta, x \in \R^+\), it can be concluded that
				\[
					y_1 = x^\alpha\cos(\beta\ln{x}) \qquad \text{and} \qquad
					y_2 = x^\alpha\sin(\beta\ln{x})
				\]
				constitute a fundamental set of real solutions. The general solution is therefore
				\[y = C_1x^\alpha\cos(\beta\ln{x}) + C_2x^\alpha\sin(\beta\ln{x})\]
	\section{Green's Function}
		\subsection{Initial-Value Problems}
			\subsubsectionb{Three Initial-Value Problems}
				Consider the second-order IVP
					\[y'' + P(x)y' + Q(x)y = f(x), \quad y(x_0) = y_0, \quad y'(x_0) = y_1\]
					The solution \(y\) can be expressed as the superposition of two solutions
					\[y(x) = y_h(x) + y_p(x)\]
					where \(y_h(x)\) is the solution of the associated homogenous DE with nonhomogenous initial conditions
					\[y'' + P(x)y' + Q(x)y = 0, \quad y(x_0) = y_0, \quad y'(x_0) = y_1\]
					and \(y_p\) is the solution of the nonhomogenous DE with homogenous (0) initial conditions
					\[y'' + P(x)y' + Q(x)y = f(x), \quad y(x_0) = 0, \quad y'(x_0) = 0\]
					\callout{15.18}{
						It is assumed that at least one of the numbers \(y_0\) or \(y_1\) is not 0. Otherwise, the \(y_h = 0\).
					}
					If the coefficients \(P\) and \(Q\) are constants, \(y_h\) can be found without issue using its auxiliary equation. \\
					Due the initial conditions being 0, \(y_p\) can describe a physical system that is initially at rest, giving it the moniker \textbf{rest solution}.
			\subsubsectionb{Green's Function}
				If \(y_1(x)\) and \(y_2(x)\) form a fundamental set of solutions on \(I\) of the associated homogenous form of the IVP, then a particular solution of the nonhomogenous equation on the interval of the form
					\[y_p(x) = u_1(x)y_1(x) + u_2(x)y_2(x)\]
					can be found on by variation of parameters. \\
					The variable coefficients are defined by
					\[
						u_1'(x) = -\frac{y_1(x)f(x)}{W}, \quad u_2'(x) = \frac{y_1(x)f(x)}{W}
					\]
					The linear independence of \(y_1\) and \(y_2\) on \(I\) guarantees that \(W(y_1(x), y_2(x)) \ne 0\) for all \(x\) in \(I\). If \(x\) and \(x_0\) are both in \(I\), the integrating the derivatives on the interval \([x_0, x]\) and substituting the results into \(y_p\) yields
					\[y_p(x) = -y_1(x)\int_{x_0}^x\frac{y_2(t)f(t)}{W(t)}\dd{t} + y_2(x)\int_{x_0}^x\frac{y_1(t)f(t)}{W(t)}\dd{t}\]
					As \(y_1(x)\) and \(y_2(x)\) are constant with respect to \(t\),
					\[y_p(x) = -\int_{x_0}^x\frac{y_1(x)y_2(t)}{W(t)}f(t)\dd{t} + \int_{x_0}^x\frac{y_1(t)y_2(x)}{W(t)}f(t)\dd{t}\]
					These two integrals can be rewritten as the single integral
					\[y_p(x) = \int_{x_0}^x G(x, t)f(t)\dd{t}\]
					where
					\[G(x, t) = \frac{y_1(t)y_2(x) - y_1(x)y_2(t)}{W(t)}\]
					is the \textbf{Green's function} for the DE. \\
				Note that a Green's function is dependent only on the fundamental solutions \(y_1\) and \(y_2\) of the associated homogenous DE of the IVP and \textit{not} on the forcing function \(f\). Therefore all linear second-order DEs with the same left-hand side but with different forcing functions will have the same Green's function. An alternative title for the Green's function is the \textbf{Green's function for the second-order differential operator \(\bm{D^2 + P(x)D + Q(x)}\)}.
		\subsection{Boundary Value Problems}
			A BVP for a second-order DE specifies \(y\) and \(y'\) at two different points. Conditions for BVPs are special cases of the more general homogenous boundary conditions
				\begin{align*}
					A_1y(a) + B_1y'(a) &= 0 &
						A_2y(b) + B_2(b) &= 0
				\end{align*}
				where \(A_1\), \(A_2\), \(B_1\), and \(B_2\) are constants. The goal is to find the integral solution \(y_p(x)\) for the nonhomogenous BVP of the form
				\begin{align*}
					y'' + P(x)y' + Q(x)y &= f(x) \\
					A_1y(a) + B_1y'(a) &= 0 \\
					A_2y(b) + B_2y'(b) &= 0
				\end{align*}
				It is assumed that \(P(x)\), \(Q(x)\), and \(f(x)\) are all continuous on \([a, b]\) as well as that the corresponding homogenous BVP has only the trivial solution \(y = 0\). This latter assumption is sufficient to guarantee that a unique solution of the nonhomogenous BVP exists and is given by
				\[y_p(x) = \int_a^b G(x, t) \dd{t}\]
			\subsubsectionb{Another Green's Function}
				Let \(y_1(x)\) and \(y_2(x)\) be linearly independent solutions on \([a, b]\) of the associated of the associated homogenous BVP at \(x\) be within the interval. These can be integrated over two different intervals:
					\begin{align*}
						u_1(x) &= -\int_b^x \frac{y_2(t)f(t)}{W(t)} \dd{t} &
							u_2(x) &= \int_a^x \frac{y_1(t)f(t)}{W(t)} \dd{t}
					\end{align*}
					A particular solution is then
					\[y_p(x) = \int_a^x \frac{y_1(x)y_2(t)}{W(t)}f(t) \dd{t} + \int_x^b \frac{y_1(t)y_2(x)}{W(t)}f(t) \dd{t}\]
					This can be compactly written as
					\[y_p(x) = \int_a^b G(x, t) f(t) \dd{t}\]
					where
					\[
						G(x, t) = \begin{cases}
 							\frac{y_1(t)y_2(x)}{W(t)} & a \le t \le x \\
 							\frac{y_1(x)y_2(t)}{W(t)} & x \le t \le b
 						\end{cases}
 					\]
 					This piecewise-defined function is called a \textbf{Green's function} for the BVP. It can be proven that this function is continuous on \([a, b]\). \\
 					If the solutions \(y_1\) and \(y_2\) are chosen such that at, \(x = a\)
 					\[
 						A_1y_1(a) + B_1y_1'(a) = 0\qquad \text{and} \qquad
 						A_2y_2(b) + B_2y_2'(b) = 0
 					\]
 					then \(y_p\) satisfies both homogenous boundary conditions.
	\section{Solving Systems of DEs by Elimination}
	\section{Nonlinear Differential Equations}
\end{document}
