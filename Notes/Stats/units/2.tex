\documentclass[../AP_Statistics.tex]{subfiles}

\begin{document}
	\chapter{Two-Variable Quantitative Data}
		A \textbfi{response} (\emphi{dependent}) \textbfi{variable} measures the outcome while a \textbfi{explanatory} (\emphi{independent}) \textbf{variable} explains the data. \\
		The term \textbf{bivariate data}\index{data!bivariate} refers to data with two variables that are recorded for the same set of \emphi{individuals}. \\
		A \textbfi{scatterplot} is, as the name suggests, a set of points, each representing an individual, scattered about a grid with $x$ and $y$ axes that correspond to the response and explanatory variables respectively.\footnote{To plot the data of two lists as a scatterplot, \texttt{stat plot} (\texttt{2nd/stat plot})) can be pressed, and then a plot selected and toggled \texttt{On}, its first option selected for type, and the \texttt{X} and \texttt{Ylist} set to their corresponding lists. To cycle through individuals in \texttt{Xlist} index order, it \texttt{trace} can be selected.} They are the most effective way for the relationship between two quantitative variables\index{variable!quantitative} measured on the same individuals.
		\callout{18.7}{When storing data within lists, it is crucial that each element $x_n$ and $y_n$ correspond to the same individual.}
		A scatterplot can be described by its \emphi{form}, \emphi{direction}, and \emphi{strength}. \\
		\textbf{Form} describes the nature (\pindex{shape}) of the variable's relationships (linear, polynomial, root, exponential, logarithmic, sinusoidal, etc.). More generally, form can be described as linear\index{form!index} or nonlinear\index{form!nonlinear}.
		\textbf{Strength} describes how strongly correlated the variables are. \\
		\textbf{Association direction}\index{association!direction} describes whether a positive change in the explanatory variable results in an increase (positive) or decrease (negative) in the explanatory variable.
			\section{Correlation}
			The \textbf{correlation coefficient}\index{correlation!coefficient} $\pmb{r}$ is equal to the sum of the products of the $z$-scores\index{z-score} of each variable for each individual divided by one less than the number of individuals.
			\[r = \frac{1}{n - 1}\sum\left(\frac{x_i - \bar{x}}{s_x}\right)\left(\frac{y_i - \bar{y}}{s_y}\right) = \frac{\sum z_{xi}z_{yi}}{n - 1}\]
			The magnitude of $r$ corresponds to strength\index{correlation!strength} while its sign indicates the \emph{association direction}. \\
			\callout{17}{
				The following should be noted regarding $r$:
				\begin{itemize}
					\item It is the same for $x$ against $y$ and $y$ against $x$ (making it \emph{commutative}).
					\item It requires that both variables be quantitative\index{variable!quantitative}.
					\item It is independent of the units of the variables. 
					\item Its magnitude cannot exceed 1.
					\item It only measures the strength of \emph{linear} relationships.
						\callout{12.6}{The correlation may be 0 even when there is a strong nonlinear pattern.}
					\item It is not \emphi{resistant}, as its formula includes non-resistant means and standard deviations, meaning that it is prone to being affected by outliers.
					\item It does not completely describe bivariate data\index{data!bivariate}. 
						\callout{10.42}{A strong correlation\index{correlation} alone is not enough to ensure \pindex{linearity}.}
					\item It is not affected by \emph{linear transformations}\index{transformations!linear} of either variable, even if they are not transformed correspondingly.
				\end{itemize}
			}
		\section{Regression and Residuals}
			The \textbf{line of best fit}\index{best fit!line} of a \pindex{scatterplot} is the line that best predicts the \pindex{data}. A method for finding a linear line of best fit is \textbf{least-squares regression}\index{regression!least-squares}.\footnote{On a calculator, a linear regression\index{regression!linear} can be carried out by entering the data into two corresponding lists. From there, \texttt{LinReg(ax+b)} (\texttt{stat/CALC/4}) with the appropriate \texttt{X} and \texttt{Ylist}s entered. This also calculates the $r$ value. To store the equation, \texttt{Store RegEQ} can be set to $\texttt{Y}_n$. Before a regression is run, though, \texttt{DiagnosticOn} (\texttt{2nd/catalog/DiagnosticOn}) should be selected.} The line generated by this method is referred to as the \textbf{least-squares regression line} or \textbf{LSRL}. \\
			The LSRL enables predictions to be made regarding how the response variable when the explanatory variable is changed. It is defined as the line the minimizes the sum of the squares of the \textbf{residuals}\index{residual} (denoted $\pmb{e}$), the differences between the predicted and actual values of the response variable\index{variable!response}.\footnote{A list of residuals can be created by setting $\texttt{L}_n$ to \texttt{RESID} (\texttt{2nd/list/7}), though the \pindex{regression} should first be run.} \\
			A \textbf{residual plot}\index{plot!residual} shows the residuals of each data point. A random pattern suggests linearity.\footnote{This can be found on a calculator by setting \texttt{Ylist} to \texttt{RESID} (\texttt{2nd/list/7}) when creating a scatterplot. This should be done after running a regression.} \\
			The regression line of a residual plot is always $\hat{y} = 0$, as the sum of all residuals is equal to 0. This also means that the mean of the residuals is equal to 0. \\
			Due to the fact that it uses a sum of every point's values, the LSRL is not resistant to \textbf{influential observations}\index{observation!influential}. Influential observations are often outliers\index{outlier} in the $y$ direction, especially in smaller data sets. To identify whether a point is an outlier, a modified box plot can be created. A point is influential if removing it markedly changes $r$. \\
			The predicted value for the explanatory variable is denoted by putting a \enquote{hat} over the variable.
			The LSRL can be described as a linear transformation that maps $x$ onto $y$:
			\[\hat{y} = a + bx\]
			Points with large residuals are \emph{outliers}. \\
			Predictions made within the data are \textbf{interpolations}\index{interpolation} while those made outside the bounds of the data are \textbf{extrapolations}\index{extrapolation}.
			\callout{15.85}{Extrapolation should be avoided, as it is more likely not to be accurate than interpolation.}
			$a$ and $b$ can be calculated as such:\footnote{
				$a$ and $b$ can be derived by differentiating the sum of the squares of the residuals.
				\[E = \sum(y_i - \hat{y})^2 = \sum(y - a - bx_i)\]
				Because the \pindex{error} is being minimized, its derivatives are equal to 0.
				\begin{align*}
					\partial_a E &= \partial_a\sum(y_i - a - bx_i)^2 & \partial_b E &= \partial_b\sum(y_i - a - bx_i)^2\\
					0 &= \sum2(y_i - a - bx_i)^2(\partial_a[y_i - a - bx_i])  & 0 &= \sum2(y_i - a - bx_i)(\partial_b[y_i - a - bx_i])\\
					&= \sum(y_i - a - bx_i)(-1) &&= \sum(y_i - a - bx_i)(-x_i) = \sum(x_iy_i - x_i\bar{y} + b\bar{x}x_i - bx_i^2)\\
					&= \sum y_i - \sum a - \sum bx_i = \sum y_i - an - b\sum x_i &&= \sum(x_iy_i - x_i\bar{y}) - b\sum(x_i^2 - \bar{x}x_i)\\ 
					a &= \frac{\sum y_i - b\sum x_i}{n} = \bar{y} - b\bar{x} & b&= \frac{\sum(x_iy_i - \bar{y}x_i)}{\sum(x_i^2 - \bar{x}x_i)} = \sum\left(\frac{y_i - \bar{y}}{x_i - \bar{x}}\right) =  \sum\left(\frac{(x_i - \bar{x})(y_i - \bar{y})}{(x_i - \bar{x})^2}\right)\\ 
					&&&= \frac{1}{s_x(n - 1)}\sum\left(\frac{(x_i - \bar{x})(y_i - \bar{y})}{s_x}\right) \\
					&&&= \frac{1}{n - 1}\sum\left(\frac{x_i - \bar{x}}{s_x}\right)\left(\frac{y_i - \bar{y}}{s_y}\right) \times \frac{s_y}{s_x} = r\frac{s_y}{s_x}
				\end{align*}
			}
			\begin{align*}
				a &= \bar{y} - bx & b &= r\frac{s_y}{s_x}
			\end{align*}
			The definition of $a$ means that the LSRL always passes through $(\bar{x}, \bar{y})$\index{mean}.\\
			The \textbf{coefficient of determination}\index{determination!coefficient} $\pmb{r^2}$ is the percentage of the change/variation in the response variable\index{variable!response} that can be explained by the LSRL\index{regression!least-squares} relating the response variable to the explanatory variable\index{variable!explanatory}. It is maximized by the LSRL.\footnote{Give no information regarding $x$, the predicted value of $y$ is simply $\bar{y}$, so the residual will be the difference between $y$ and $\bar{y}$. The sum of all residuals must therefore be 0:\[\sum e_i = \sum(y_i - \bar{y}) = \sum y_i - \sum \bar{y} = \sum y_i - n\bar{y} = \sum y_i - \frac{n\sum 
			y_i}{n} = - \sum y_i - \sum y_i = 0\]Because the sum of the residuals is 0, the sum of the squares of the residuals can instead be used to compare errors between approximations. $r^2$ is equal to the percentage of error removed by using the LSRL rather than $\hat{y} = \bar{y}$.} \\
			\textbf{Clusters} \index{cluster} are groups of points that are similar. \\
			To add a categorical variable\index{variable!categorical}, the data can be displayed using different symbols for each group. \\
		\section{Transformations to Achieve Linearity}
			The form of a relationships is not always linear, but the LSRL calculates a linear line of best fit\index{best fit!line!linear}. To resolve this, $x$ and/or $y$ can be transformed\index{transformation!to achieve linearity} using powers, roots, or logarithms. This process is known as \textbf{linearization}. \\
			The axes of the \pindex{scatterplot} are changed in accordance with the \pindex{linearization}. \\
			An \textbf{exponential model}\index{model!exponential} takes the form of an exponential function.
			\[\hat{y} = ab^x\]
			A \textbf{logarithmic model}\index{model!logarithmic} takes the form of a natural logarithmic function.
			\[\hat{y} = a + b\ln x\]
			\callout{16.23}{Exponentiating one variable and taking the logarithm of the other are equivalent operations.}
			A \textbf{power model}\index{model!power} takes the form of a polynomial with direct variation.
			\[\hat{y} = ax^b\]
			Taking the logarithm of both sides simply results in a power model after it is re-expressed.
			\begin{align*}
				\ln \hat{y} &= a + b\ln x \\
				e^{\ln \hat{y}} &= e^{a + b\ln x} \\
				\hat{y} &= e^a e^{\ln x^b} = cx^b
			\end{align*}
			The model that bests fits the \pindex{data} is the one that minimizes $s$, the standard deviation of the residuals\index{standard deviation}.
	\chapter{Two-Variable Categorical Data}\index{data!categorical}
		\textbf{Segmented bar graphs} have one bar per \pindex{variable}. Each bar is divided into segments according to their \emph{relative frequency}\index{frequency!relative}. The height of each bar is equal to 1. \\
		\textbf{Side-by-side bar graphs} show the distribution of one categorical variable for each value of another. The grouping of the bars is based on one of the categorical variables while the bars themselves show the frequencies of the values of the other. \\
		\textbf{Mosaic plots} are variants of segmented bar graphs that display the number of individuals in a category by making each bar's width proportional to it. \\
		A \textbf{two-way table} shows data on the relationship between two categorical variables for some group of individuals. \\
		There are 3 types of \emph{relative frequencies}:
		\begin{enumerate}
			\item \textbf{Marginal relative frequency}\index{frequency!relative!marginal} is the proportion of individuals with a specific value for one categorical variable.
				\[\text{marginal relative frequency} = \frac{n}{N}\]
			\item \textbf{Joint relative frequency}\index{frequency!relative!joint} is the proportion of individuals with a specific value for one categorical variable as well as a specific value for another categorical variable.
				\[\text{joint relative frequency} = \frac{n_{A\land B}}{N}\]
			\item \textbf{Conditional relative frequency}\index{frequency!relative!conditional} is the proportion of individuals that have a specific value for one variable that also have a specific value for another variable.
				\[\text{conditional relative frequency} = \frac{n_A}{n_B}\]
		\end{enumerate}
			If knowing the value of one variable helps in predicting that of another, there is an \emphi{association} between them. To test for association, the marginal and conditional relative frequencies can be compared. If they vary \emph{significantly}, there is an association.
\end{document}