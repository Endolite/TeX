\documentclass[../AP_Statistics.tex]{subfiles}

\begin{document}
	\chapter{Confidence Intervals}
		A statistic is a \textbf{point estimator} used to estimate an unknown population parameter. An arbitrary parameter and its point estimator are denoted $\theta$ and $\hat{\theta}$ respectively. \\
		A \textbf{point estimate} is a statistic's value, and is referred to as such because it is a single point. As such, it is almost never accurate. \\
		A point estimate can be made made more reliable by either increasing the sample size or using a more accurate sampling procedure. \\
		The \textbf{standard error $\bm{s}$} (with the appropriate subscript denoting its statistic) of a statistic is the point estimate of the standard deviation of the sampling distribution. \\
		A \textbf{confidence interval} provides an interval of plausible values for an unknown parameter based on sample data. It is equal to the point estimate plus or minus the \textbf{margin of error (ME)}.
		\[\cint = \pest \pm \moe = \hat{\theta} \pm \ME\]
		The probability that a confidence interval contains the true parameter value is the confidence interval's \textbf{confidence level ($\bm{C}$)}.
		\[P(\p \in \cint) = P(\theta \in \hat{\theta} \pm \ME) = C\]
		The margin of error describes the maximum deviation of the estimate from the parameter. It is the product of the critical value and the standard error of the statistic.
		\[\ME = \crit \times \text{ standard error of statistic} = \crit  \times s_{\hat{\theta}}\]
		The \textbf{critical value} is equal to the the number of standard deviations from the mean within which the probability of a random variable falling is equal to $C$.
		\[P(-\crit < \sts< \crit) = C\]
		\section{Confidence Intervals about Proportions}
			%TODO Confidence Intervals about Proportions
			\subsection*{Confidence Intervals about Differences in Proportions}
		\section{Confidence Intervals about Means}
			In order for Normality to be verified, the central limit theorem can be used, necessitating that $n$ be at least 30, or a modified box plot can be created with the data and observed to be symmetrical without outliers. \\
			When the standard deviation of $X$ (not of the sampling distribution of $\bar{x}$) is known, a confidence interval about $\bar{x}$ can be constructed using the templates for confidence intervals and margin of error, simply using the sampling distribution's standard deviation rather than the statistic's standard error.
			\[\cint = \bar{x} \pm z^*\frac{\sigma}{\sqrt{n}}\]
			This is a \textbf{one-sample $\bm{z}$-interval for a population mean}.
			When $\sigma$ is not known, the population standard deviation can be replaced by its point estimator to calculate the \textbf{standard error of $\bm{\bar{x}}$}.
			\[s_{\bar{x}} = \frac{s_x}{\sqrt{n}}\]
			The margin of error is appropriately changed:
			\[\ME = z^*\frac{s_x}{\sqrt{n}}\]
			This results in the intervals being too small, though, and the confidence level decreases from what would be expected given $z^*$. The critical value can also change, though, becoming $t^*$ rather than $z^*$, making the intervals longer and making the confidence level representative. The reason that $t$ is used is that a $\bm{t}$\textbf{ distribution} is used rather than a Normal one. ($t^*$ can still be interpreted in the same way as $z^*$, though (the number of standard errors from the point estimate).) \\
			The specific $t$ distribution used is specified by \textbf{degrees of freedom (df)}, which  is 1 less than the sample size.\footnote{The density curve of a $t$ distribution with degrees of freedom $\nu$ is defined (using the gamma function $\Gamma$ or the beta function $\B$) as such:\[f(x) = \frac{\Gamma\left(\frac{\nu + 1}{2}\right)}{\sqrt{\nu\pi}\Gamma\left(\frac{\nu}{2}\right)}\left(1 + \frac{x^2}{\nu}\right)^{-\frac{\nu + 1}{2}} = \frac{1}{\sqrt{\nu}\B\left(\frac{1}{2},\frac{\nu}{2}\right)}\left(1 + \frac{x^2}{\nu}\right)^{-\frac{\nu + 1}{2}}\]As $\df$ approaches infinity, the $t$ distribution approaches a normal curve (the tails approaching 0 more quickly), so $t^*$ approaches $z^*$. This is because a greater sample size means that $s_x$ will be closer to $\sigma$.} \\
			\[\df = n - 1\]
			The confidence interval constructed about $\bar{x}$ using $t^*$ and $s_x$ is a \textbf{one-sample $\bm{t}$ interval for a population mean}.
			\[\ME = t^*\frac{s_x}{\sqrt{n}}\]
			Because $t^*$ is dependent on $\df$, which is 1 less than the sample size, and $s_x$ is dependent on the data, which has not been produced, so the sample size cannot be solved for given a confidence level and the margin of error, so $z^*$ and $\sigma$, a value of $s_x$ from a previous study are instead used.
			\begin{align*}
				\ME &\ge z^*\frac{\sigma}{\sqrt{n}} \\
				n &\ge \left(\frac{\ME}{z^*}\right)^2
			\end{align*}
			\subsection*{Confidence Intervals about Differences in Means}
				A confidence interval about a difference in means is a \textbf{two-sample $\bm{t}$ interval for a mean difference}. In order for it be constructed about, Normality and independence must be justified and both samples must be independent. \\
				The center of the confidence interval is the difference between the sample means while its standard error is simply the square root of the sum of the variances of the standard errors of the individual statistics.
				\[\cint = (x_1 - x_2) \pm \msdiff{s}{1}{2} = (\bar{x}_1 - \bar{x}_2) \pm t^*\msdiffe{s}{1}{n_1}{2}{n_2}\]
				If both standard deviations are known, they are used along with $z^*$.
				\[(\bar{x}_1 - \bar{x}_2) \pm z^*\sqrt{\frac{\sigma_1^2}{n_1} +\frac{\sigma_2^2}{n_2}}\]
				The degrees of freedom of a difference of means is the equal to the square of the sample variance of the difference divided by the sum of the those of each statistic divided by 1 less than their sample sizes.
				\[\df = \frac{s_{\bar{x}_1 - \bar{x}_2}^4}{\frac{s_{\bar{x}_1}^4}{n_1 - 1} + \frac{s_{\bar{x}_2}^4}{n_2 - 1}} = \frac{\left(\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}\right)^2}{\frac{\left(s_1^2/n_1\right)^2}{n_1 - 1} + \frac{\left(s_2^2/n_2\right)^2}{n_2 - 1}}\]
				When the data is \emph{paired}, the response being collected from the same set of individuals independently, the difference can be treated as a single mean, as each sample difference is known, and a \textbf{paired $\bm{t}$ interval} can be created. The center of this confidence interval is the mean of the differences, denoted $\diff{\bar{x}}$, while the standard error is derived from the sample standard deviation of the differences.\footnote{
					When $n_1 = n_2$, it can be verified that the values of $s_{\bar{x}_1 - \bar{x}_2}$ derived by $\diff{s}$ and by $s_1$ and $s_2$ individually are the same.
					\begin{align*}
						\diff{s} &= \sqrt{s_1^2 + s_2^2} \\
						\msdiff{s}{1}{2} &= \frac{\diff{s}}{\sqrt{n}} & &= \msdiffe{s}{1}{n}{2}{n} \\
							&= \sqrt{\frac{s_1^2 + s_2^2}{n}} & &= \sqrt{\frac{s_1^2 + s_2^2}{n}}\\
					\end{align*}
					}
				\begin{align*}
					\diff{\bar{x}} &= \overline{x_1 - x_2} = \bar{x}_1 - \bar{x}_2 & \diff{s} &= s_{1 - 2} = \sqrt{s_1^2 + s_2^2} &\cint = \diff{\bar{x}} \pm t^*\frac{\diff{s}}{\sqrt{n}}
				\end{align*}
	\chapter{Significance Tests}
		A \textbf{significance test} is a procedure that uses observed data to test between two claims, often made regarding parameters, about hypotheses.
		\callout{17}{In order for a significance test to be performed, randomness, independence, and Normality must be verified.}
		The \textbf{null hypothesis $\bm{H_0}$} claims that the parameter is equal to a \textbf{null value}, what it was previously assumed to be, denoted by a subscript $0$ on the parameter. It is often a statement of no change or difference.
		\[H_0: \p = \np\]
		The claim that is attempting to be supported is the \textbf{alternate hypothesis ($\bm{H_a}$)}. It can either be \textbf{one-sided}, claiming that the parameter is greater or less than the null value, or \textbf{two-sided}, claiming simply that the parameter is not equal to the null value.
		\begin{align*}
			H_a&: \p \gtrless \np & H_a: \p \ne \np
		\end{align*}
		A test's $\bm{P}$\textbf{-value} is the probability of \emph{significant} evidence being found that supports $H_a$ that is at least as strong as that observed assuming that $H_0$ is true.
		\[\pval = P(\text{statistic supports } H_a \mid H_0)\] \\
		The smaller the $\pval$, the lower the chances of receiving evidence of the alternative. A small $\pval$ therefore supports the $H_a$. \\
		If the $\pval$ is less than the \textbf{significance level} $\bm{\alpha}$, $H_0$ can be rejected and it can be concluded that there is convincing evidence for $H_a$. If the $\pval$ is greater than or equal to $\alpha$, $H_0$ cannot be rejected, and it can be concluded that there is not convincing evidence for $H_a$. \\
		The $\pval$ is calculated using the \textbf{standardized test statistic} (t), which is the number of standard deviations from the null value of the parameter. \\
		For a null hypothesis to be significant is for a significance test to provide a $\pval$ less than the significance level.
		\[\sts = \z{\s}{\np}{\text{standard error of statistic}}\]
		The $\pval$ is equal to the probability of $z$ satisfying the $H_a$ assuming that $H_0$ is true. It can therefore be calculated as such using a \emph{cumulative distribution function}, so long as Normality and independence are justified.
		\[\small
			\pval = \begin{cases}
				P(\s \gtrless \os \mid H_0) & H_a: \p \gtrless \np \\
				P(|\s| > |\os| \mid H_0) & H_a:\text{parameter} \ne \text{null parameter}
			\end{cases}
		\]
		\callout{17}{Conclusions should only ever be made regarding the rejection of $H_0$ and the convincing support of $H_a$. $H_0$ should never be supported and $H_a$ should never be rejected.}
		\callout{19}{
				When answering a question regarding a significance test, the following process can be followed:
				\paragraph{1. State} 
					State the hypotheses to be tested and the significance level and define any parameters.
				\paragraph{2. Plan}
					Identify the appropriate methods of inference and verify its conditions.
				\paragraph{3. Do}
					State the sample statistic(s), calculate the standardized test statistic(s), and calculate the $\pval$.
				\paragraph{4. Conclude}
					Make a conclusion regarding the hypotheses within the problem's context.
			}
		When performing significance tests, two types of errors may occur:
		\begin{itemize}
			\item
				A \textbf{Type \Roman{1} error} occurs when $H_0$ is rejected despite $H_a$ being false; the data provided convincing evidence for $H_a$ despite it being incorrect. 
			\item
				A \textbf{Type \Roman{2} error} occurs when $H_0$ is not rejected despite $H_a$ being true; the data did not provide convincing evidence for $H_a$ despite it being correct.
		\end{itemize}
		\begin{center}
			\begin{tabular}{cc}
			&\begin{tabular}{cc}$H_a$ is false&\hspace{1.6cm}$H_a$ is true\end{tabular} \\
			\begin{tabular}{r}$H_0$ is rejected\\$H_0$ is not rejected\end{tabular} & \begin{tabular}{|c|c|}\hline Type \Roman{1} error&Correct conclusion\\\hline Correct conclusion&Type \Roman{2} error\\\hline\end{tabular}
		\end{tabular}
		\end{center}
		The probability of a Type \Roman{1} error occurring is equal to $\alpha$. \\
		As $\alpha$ increases, the probability of a Type \Roman{1} error increases but that of a Type \Roman{2} error decreases. \\
		A confidence interval about a statistic (using the \emph{standard error}) can be used in tandem with a sample statistic to provide a set of plausible values for the true parameter, should the alternative hypothesis be convincingly supported. \\
		A two-sided test of of $H_0$ at significance level $\alpha$ usually provides the same conclusion as a confidence level of the complement of $\alpha$.
		\[[P(\mathrm{|\s| < |\os|}) < \alpha] \approxident [\text{null parameter} \in (\text{statistic} \pm \ME)]\]
		A test's \textbf{power} is the probability of convincing evidence being found that convincingly supports $H_a$ given a value for the parameter being tested. This is also is equal to the probability of avoiding a Type \Roman{2} error. 
		\[\mathrm{power} = 1 - P(\text{Type \Roman{2} Error})^C = P(\text{statistic convincingly supports } H_a \mid H_a \text{ is true})\]
		Power can be increased in three ways:
		\begin{enumerate}
			\item 
				Increasing the sample size
				\begin{itemize}
					\item 
						A large sample means that more data is collected and more information is given regarding the true population parameter. This also increases $n$, which decreases the standard error of the statistic, reducing the value of the standardized test statistic and therefore the $\pval$, making it more likely to fall below $\alpha$.
				\end{itemize}
			\item 
				Increasing the significance level
				\begin{itemize}
					\item 
						Increasing the significance level increases the probability of $H_0$ being rejected when $H_a$ is true, as the maximum $\pval$ for $H_0$ to be rejected increases.
				\end{itemize}
			\item 
				Increasing the \textbf{effect size}, the minimum difference between the null parameter value and the alternative parameter value for the change to matter
				\begin{itemize}
					\item 
						Increasing the size of the difference that needs to be detected makes that difference more likely to be detected, as larger differences are easier to detect.
				\end{itemize}
		\end{enumerate}
		\section{Significance Tests about Proportions}
			\callout{17}{
				In order for a significance test of $H_0:p = p_0$ to be performed, the following must be verified:
				\begin{itemize}
					\item
						Randomness (Sampling or Assignment)
					\subitem
						Independence (10\% for Samples)
					\item
						Normality (Large Counts)
				\end{itemize}
			}
			To perform \textbf{1-proportion $\bm{z}$ test}, a significance test about one proportion, $z$ must be calculated. \\
			\[z = \frac{\hat{p} - \mu_{\hat{p}}}{\sigma_{\hat{p}}} = \frac{\hat{p} - p_0}{\sqrt{\frac{p_0(1 - p_0)}{n}}}\]			
			\subsection*{Significance Tests about Differences in Proportions}
				A \textbf{2-proportion $\bm{z}$ test} can be performed to compare the proportions for two populations is based on the difference between sample proportions. They hypotheses for these tests typically take the following forms:
				\begin{align*}
					H_0:p_1 - p_2 = p_0 && H_a: p_1 - p_2 \gtrless p_0 && H_a:p_1 - p_2 \ne p_0
				\end{align*}
				Typically, $p_0$ is 0, so these hypotheses can be rewritten.
				\begin{align*}
					H_0:p_1 = p_2 && H_a:p_1 \gtrless p_2 && H_a:p_1 \ne p_2
				\end{align*}
				A significance test first assumes that the null hypothesis $H_0:p_1 = p_2$ is true. This common value is referred to as $p$. \\
				The \textbf{combined sample proportion} is denoted $\hat{p}_C$ and is equal to the total successes divided by the total sample size, making it effectively a weighted average. It is the sample proportion that assumes that the parameter values are equal.
				\[\hat{p}_C = \frac{x_1 + x_2}{n_1 + n_2} = \frac{n_1\hat{p}_1 + n_2\hat{p}_2}{n_1 + n_2}\]
				The \emph{Large Counts} condition must be met with $\hat{p}_C$.
				\[n_1\hat{p}_C, n_1(1 - \hat{p}_C), n_2\hat{p}_C, n_2(1 - \hat{p}_C) \ge 10\]
				\callout{17}{
					For a significance test to be run about a difference of proportions, the randomness, independence (\emph{10\%}) (for each proportion), and \emph{Large Counts} conditions must be met.
				}
				The \emph{standardized test statistic} is the $z$-score calculated using the difference in proportions and its standard error assuming the mean to be 0 ($H_0$ to be true).
				\[z = \z{\hat{p}_1 - \hat{p}_2}{\mu_{\hat{p}_1 - \hat{p}_2}}{s_{\hat{p}_1 - \hat{p}_2}} = \z{\hat{p}_1 - \hat{p}_2}{0}{\propsed{\hat{p}_C}{n_1}{\hat{p}_C}{n_2}} = \z{\hat{p}_1}{\hat{p}_2}{\propsee{\hat{p}_C}{n_1}{n_2}}\]
		\section{Significance Tests about Means}
			\callout{17}{To perform a significance test for a population mean, a \textbf{1-sample $\bm{t}$ test}, randomness, independence (\emph{10\%}), and Normality (\emph{CLT} or distribution) must be verified.}
			The \emph{standardized test statistic} for a significance test about a mean is $t$.
			\[t = \z{\bar{x}}{\mu_{\bar{x}}}{s_{\bar{x}}} = \z{\bar{x}}{\mu_0}{s_x/\sqrt{n}}\]
			The \emph{$t$-distribution} used to calculate the $\pval$ uses \emph{degrees of freedom} 1 less than the sample size.
			\[\df = n - 1\]
			\callout{17}{Minute, practically unimportant changes in in $\mu_0$ can drastically shrink the $\pval$ when the sample size is always large enough. A very large sample size results in the null hypothesis almost always being rejected. \textbf{$\bm{P}$-hacking} takes advantage of this fact.}
			\subsection*{Significance Tests about Differences in Means}
				A \textbf{2-sample $\bm{t}$ test} about a difference in means compares the means of two populations based on samples. These tests' hypotheses take the following forms:
				\begin{align*}
					H_0: \mu_1 - \mu_2 = \mu_0 && H_a: \mu_1 - \mu_2 \gtrless \mu_0 && H_a: \mu_1 - \mu_2 \ne \mu_0
				\end{align*}
				Typically, the null difference is 0, so these hypotheses can be rewritten in the following forms:
				\begin{align*}
					H_0: \mu_1 = \mu_2 && H_a: \mu_1 \gtrless \mu_2 && H_a: \mu_1 \ne \mu_2
				\end{align*}
				\callout{17}{In order for a significance test to be performed about a difference in means, the randomness, independence (\emph{10\%}), and normality (\emph{CLT} or distribution) must be verified for \emph{each} sample.}
				The \emph{standardized test statistic} for a difference in means is as follows:
				\[t = \z{\diff{\bar{x}}}{\mu_0}{\msdiff{s}{1}{2}} = \z{\bar{x}_1 - \bar{x}_2}{0}{\msdiffe{s}{1}{n_1}{2}{n_2}} = \z{\bar{x}_1}{\bar{x}_2}{\msdiffe{s}{1}{n_1}{2}{n_2}}\]
				When the data is \emph{paired}, a \textbf{1-sample $\bm{t}$ test for a difference in means} (a \textbf{paired $\bm{t}$ test for a difference in means}) can be performed:
				\begin{align*}
					t &= \z{\diff{\bar{x}}}{\mu_0}{\msdiff{s}{1}{2}} = \z{\diff{\bar{x}}}{0}{\diff{s}/\sqrt{n}} = \frac{\diff{\bar{x}}}{\diff{s}/\sqrt{n}} & \df &= n - 1
				\end{align*}
	\chapter{Chi-Square Tests}
		\section{Goodness of Fit}
			The \textbf{chi-square test for goodness of fit} tests the null hypothesis\footnote{The hypotheses for a distribution of categorical data can be described (for an arbitrary ordering scheme of categories using index $i$) as such:\begin{align*}H_0&:(p_{o, i} = p_{e, i})\forall(i \mid \exists p_i) & H_a&:(p_{o, i} \gtrless p_{e, i})\exists(i \mid \exists p_i) & H_a &:(p_{o, i} \ne p_{e, i})\exists(i \mid \exists p_i) \end{align*}} (a theoretical distribution of \emph{categorical} data) by comparing the observed and expected counts of each category. \\
			The \textbf{chi-square test statistic} is the sum of the square of the difference between the observed and expected counts divided by the expected count over all categories.
			\[\chi^2 = \sum\left[\mathrm{\frac{(observed - expected)^2}{expected}}\right]\]
			\callout{17.1}{
				In order for a chi-square test for goodness of fit to be run, the following conditions must be met:
				\begin{itemize}
					\item
						Random (sampling or assignment)
						\subitem
							Independent (10\%)
					\item
						$\chi^2$ Distribution
						\subitem
							The expected counts in each category must be at least 5.
				\end{itemize}
			}
			For evidence to exist of a deviation from the null hypothesis is for any difference to exist between the observed and expected counts. \\
			The \textbf{expected count} is the product of the null proportion and the sample size and \emph{should not be rounded}. \\
			The \emph{degrees of freedom} is 1 less than the number of categories. \\
			A \textbf{chi-square distribution} is denoted $\bm{\chi^2(k)}$ (where $k = \df$) while its probability density function is denoted $f(x;k)$ (where $x = \chi^2$).\footnote{The probability density function of a chi-square distribution with degrees of freedom $k$ is defined as such:\[f(x;k) = \begin{cases}\frac{x^{\frac{k}{2} - 1}e^{\frac{-x}{2}}}{2^{\frac{k}{2}}\Gamma\left(\frac{k}{2}\right)} & x > 0 \\ 0 & x \le 0 \end{cases}\]}
			\begin{align*}
				\text{shape} &= \text{skew right} & \min\{\chi^2\} &= 0 & \max\{\chi^2\} &= \infty \\
				\mu_{\chi^2} &= \df & \max\{f(\chi^2;\df)\} &= f(\df - 2, \df) & \sigma_{\chi^2} &= \sqrt{2\df}
			\end{align*}
			The $\pval$ of a chi-square test is the probability of $\chi^2$ falling above the standardized test statistic.
			\[\pval = P(\Chi^2 > \chi^2)\]
			If the $\pval$ is statistically significant, the largest contributions to the test statistic can be reported by comparing the observed and expected counts. 
		\section{Homogeneity and Independence}
			The \textbf{chi-square test for homogeneity} compares the distributions of a single categorical variable for multiple populations/treatments. It is used when the totals for the explanatory variables are predetermined, as is the case for \emph{experiments}. \\
			The hypotheses for tests for homogeneity take the following forms:\footnote{The hypotheses for a test for homogeneity can be written as such:
			\begin{align*}
				H_0&: p_{1, i} = p_{2, i} \forall (i \mid \exists p_i) & H_a&: p_{1, i} \ne p_{2, i} \exists (i \mid \exists p_i)
			\end{align*}}
			\begin{align*}
				H_0&: \text{There is no difference in the true distributions of [response variable] for [different populations]} \\
				H_a&: \text{There is a difference in the true distributions of [response variable] for [different populations]}
			\end{align*}
			The \textbf{chi-square test for independence} tests the association between two categorical variables in the same population of interest. It is used when only the total is known, as is the case for \emph{samples}. \\
			The hypotheses for tests for independence take the following forms:\footnote{The hypotheses for chi square tests for independence can be written as such:
			\begin{align*}
				H_0&: P(x_i \mid y_j) = P(x_i) \forall (i, j \mid \exists x_i, y_j) & H_a&: P(x_i \mid y_j) \ne P(x_i) \exists (i, j \mid \exists x_i, y_j)
			\end{align*}}
			\[\begin{aligned}
				H_0&: \text{There is no association between [explanatory variable] and [response variable]} \\
				H_a&: \text{There is an association between [explanatory variable] and [response variable]}
			\end{aligned}\]
			\callout{18.1}{The conditions for chi-square tests for homogeneity and independence are the same as those for one for goodness of fit.}
			The \emph{expected counts} for a cell on a two-way table is the product of the sum and row totals divided by the total.
			\[\mathrm{expected = \frac{\text{row total} \times \text{column total}}{total}}\]
			The \emph{degrees of freedom} is the product of one less than the row and column totals.
			\[\mathrm{\df = (rows - 1)(columns - 1)}\]
			The chi-square statistic itself is defined the same way as for tests for goodness of fit.
	\chapter{Slopes}
		The \textbf{sample regression line} takes the following form:
		\[\hat{y} = a + bx\]
		The sample egression line can be used to test claims regarding the \textbf{population (true) regression line}, which takes the following form:
		\[\mu_y = \alpha + \beta x\]
		\callout{17}{
			In order for inference to be performed regarding a regression, the following condition must be met:
			\begin{itemize}
				\item
					The true relationship between the response and explanatory variables must be \emph{linear}.
				\item
					Individual observations must be \emph{independent (10\%)}.
				\item
					The response variable must vary approximately Normally for a given value of the explanatory variable.
				\item
					The standard deviation of the response variable must not vary with the value of the explanatory variable
				\item
					The data must be random.
			\end{itemize}
		}
		When the conditions for inference are met, the \textbf{sampling distribution of the sample slope $\bm{b}$} follows an approximately Normal distribution with the following parameters:
		\begin{align*}
			\mu_b &= \beta & \sigma_b &= \frac{\sigma_y}{\sigma_x\sqrt{n}}
		\end{align*}
		The sample slope $b$ and intercept $a$ estimate the population regression line's slope $\beta$ and intercept $\alpha$. \\
		The standard error of the sample slope can be approximated, using the sample standard deviation of the \emph{residuals} $s$ in place of that of the response variable.
		\[s_b = \frac{s}{s_x\sqrt{n - 1}}\]
		Confidence intervals and significance tests for the population slope are based on a $t$ distribution with \emph{degrees of freedom} \underline{2} less than the sample size.
		\[\df = n - 2\]
		The \textbf{$\bm{t}$ interval for the slope $\bm{\beta}$} is as follows:
		\[\cint = b \pm t^*s_b\]
		The hypotheses for a \textbf{$\bm{t}$ test about a slope} are as follows:
		\begin{align*}
			H_0&: \beta = \beta_0 & H_a&: \beta \gtrless \beta_0 & H_a&: \beta \ne \beta_0
		\end{align*}
		The null slope is typically 0, which is claiming that no correlation exists between the response and explanatory variables. \\
		The \emph{standardized test statistic $t$} can be calculated as such:
		\[t = \z{b}{\mu_b}{\sigma_b} \approx \z{b}{\beta_0}{s_b}\]
		\footnote{
				The following Minitab table is often provided and can be interpreted as such:
				\[\begin{array}{|c|c|c|c|c|}\hline
					\texttt{Predictor} & \texttt{Coef} & \texttt{SE Coef} & \texttt{T} & \texttt{P} \\\hline
					\texttt{Constant} & a & s_a = \sqrt{s\left(\frac{1}{n} + \frac{\bar{x}^2}{s_x\sqrt{n - 1}}\right)} & t_a = \frac{a}{s_a} & P(|T| > |t_a|) \\\hline
					\texttt{[Explanatory Variable]} & b & s_b = \frac{s}{s_x\sqrt{n - 1}} & t_b = \frac{b}{s_b} & P(|T| > |t_b|) \\\hline
					\texttt{S = } s & \multicolumn{2}{c|}{\texttt{R-Sq = } r^2} & \multicolumn{2}{c|}{\texttt{R-Sq(adj) = } r^2 \text{ excluding influential points}} \\\hline
				\end{array}\]}
\end{document} 