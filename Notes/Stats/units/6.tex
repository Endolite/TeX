\documentclass[../AP_Statistics.tex]{subfiles}

\begin{document}
	\chapter{Confidence Intervals}
		A statistic is a \textbf{point estimator} used to estimate an unknown population parameter. A \textbf{point estimate} is a statistic's value, and is referred to as such because it is a single point. As such, it is almost never accurate. \\
		A point estimate can be made made more reliable by either increasing the sample size or using a more accurate sampling procedure. \\
		The \textbf{standard error} $\pmb{s}$ (with the appropriate subscript denoting its statistic) of a statistic is the point estimate of the standard deviation of the sampling distribution. \\
		A \textbf{confidence interval} provides an interval of plausible values for an unknown parameter based on sample data. It is equal to the point estimate plus or minus the \textbf{margin of error} ($\pmb{\ME}$).
		\[\text{confidence interval} = \text{point estimate} \pm \text{margin of error}\]
		The probability that a confidence interval contains the true parameter value is the confidence interval's \textbf{confidence level} ($\pmb{C}$).
		The margin of error describes the maximum deviation of the estimate from the parameter. It is the product of the critical value and the standard error of the statistic.
		\[\ME = \text{critical value} \times \text{standard error}\]
		The \textbf{critical value} is equal to the the number of standard deviations from the mean within which the probability of a random variable falling is equal to $C$.
		\[P(-\text{critical value} < \text{standardized test statistic} < \text{critical value}) = C\]
		\section{Confidence Intervals about Proportions}
			%TODO Confidence Intervals about Proportions
			\subsection*{Confidence Intervals about Differences in Proportions}
		\section{Confidence Intervals about Means}
			In order for Normality to be verified, the central limit theorem can be used, necessitating that $n$ be at least 30, or a modified box plot can be created with the data and observed to be symmetrical without outliers. \\
			When the standard deviation of $X$ (not of the sampling distribution of $\bar{x}$) is known, a confidence interval about $\bar{x}$ can be constructed using the templates for confidence intervals and margin of error, simply using the sampling distribution's standard deviation rather than the statistic's standard error.
			\[\cint = \bar{x} \pm z^*\frac{\sigma}{\sqrt{n}}\]
			This is a \textbf{one-sample $\pmb{z}$-interval for a population mean}.
			When $\sigma$ is not known, the standard deviation can be replaced by the standard error to calculate the \textbf{standard error of $\pmb{\bar{x}}$}.
			\[s_{\bar{x}} = \frac{s_x}{\sqrt{n}}\]
			The margin of error is appropriately changed:
			\[\ME = z^*\frac{s_x}{\sqrt{n}}\]
			This results in the intervals being too small, though, and the confidence level decreases from what would be expected given $z^*$. The critical value can also change, though, becoming $t^*$ rather than $z^*$, making the intervals longer and making the confidence level representative. The reason that $t$ is used is that a $\pmb{t}$\textbf{ distribution} is used rather than a Normal one. ($t^*$ can still be interpreted in the same way as $z^*$, though (the number of standard errors from the point estimate).) \\
			The specific $t$ distribution used is specified by \textbf{degrees of freedom} ($\pmb{\df}$), which  is 1 less than the sample size.\footnote{The density curve of a $t$ distribution with degrees of freedom $\nu$ is defined (using the gamma function $\Gamma$ or the beta function $\B$) as such:\[f(x) = \frac{\Gamma\left(\frac{\nu + 1}{2}\right)}{\sqrt{\nu\pi}\Gamma\left(\frac{\nu}{2}\right)}\left(1 + \frac{x^2}{\nu}\right)^{-\frac{\nu + 1}{2}} = \frac{1}{\sqrt{\nu}\B\left(\frac{1}{2},\frac{\nu}{2}\right)}\left(1 + \frac{x^2}{\nu}\right)^{-\frac{\nu + 1}{2}}\]As $\df$ approaches infinity, the $t$ distribution approaches a normal curve (the tails approaching 0 more quickly), so $t^*$ approaches $z^*$. This is because a greater sample size means that $s_x$ will be closer to $\sigma$.} \\
			\[\df = n - 1\]
			The confidence interval constructed about $\bar{x}$ using $t^*$ and $s_x$ is a \textbf{one-sample $\pmb{t}$ interval for a population mean}.
			\[\ME = t^*\frac{s_x}{\sqrt{n}}\]
			Because $t^*$ is dependent on $\df$, which is 1 less than the sample size, and $s_x$ is dependent on the data, which has not been produced, so the sample size cannot be solved for given a confidence level and the margin of error, so $z^*$ and $\sigma$, a value of $s_x$ from a previous study are instead used.
			\begin{align*}
				\ME &\ge z^*\frac{\sigma}{\sqrt{n}} \\
				n &\ge \left(\frac{\ME}{z^*}\right)^2
			\end{align*}
			\subsection*{Confidence Intervals about Differences in Means}
				A confidence interval about a difference in means is a \textbf{two-sample $\pmb{t}$ interval for a mean difference}. In order for it be constructed about, Normality and independence must be justified and both samples must be independent. \\
				The center of the confidence interval is the difference between the sample means, denoted by a subscript \textbf{diff}, while its standard error is simply the square root of the sum of the variances of the standard errors of the individual statistics.
				\begin{align*}
					\diff{\bar{x}} &= \overline{x_1 - x_2} = \bar{x}_1 - \bar{x}_2 & 
					\cint &= \diff{\bar{x}} \pm \msdiff{s}{1}{2} = (\bar{x}_1 - \bar{x}_2) \pm t^*\msdiffe{s}{1}{n_1}{2}{n_2}
				\end{align*}
				If both standard deviations are known, they are used along with $z^*$.
				\[(\bar{x}_1 - \bar{x}_2) \pm z^*\sqrt{\frac{\sigma_1^2}{n_1} +\frac{\sigma_2^2}{n_2}}\]
				The degrees of freedom of a difference of means is the equal to the estimated variance divided by the sum of the estimated variances of each statistic divided by 1 less than their sample sizes.
				\[\df = \frac{s_{\bar{x}_1 - \bar{x}_2}^4}{\frac{s_{\bar{x}_1}^4}{n_1 - 1} + \frac{s_{\bar{x}_2}^4}{n_2 - 1}} = \frac{\left(\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}\right)^2}{\frac{\left(s_1^2/n_1\right)^2}{n_1 - 1} + \frac{\left(s_2^2/n_2\right)^2}{n_2 - 1}}\]
				When the data is \emph{paired}, the response being collected from the same set of individuals independently, the difference can be treated as a single mean, as each sample difference is known, and a \textbf{paired $\pmb{t}$ interval} can be created. $\diff{s}$ can therefore be used as well.\footnote{
					When $n_1 = n_2$, it can be verified that the values of $s_{\bar{x}_1 - \bar{x}_2}$ derived by $\diff{s}$ and by $s_1$ and $s_2$ individually are the same.
					\begin{align*}
						\diff{s} &= \sqrt{s_1^2 + s_2^2} \\
						\msdiff{s}{1}{2} &= \frac{\diff{s}}{\sqrt{n}} & &= \msdiffe{s}{1}{n}{2}{n} \\
							&= \sqrt{\frac{s_1^2 + s_2^2}{n}} & &= \sqrt{\frac{s_1^2 + s_2^2}{n}}\\
					\end{align*}
					}
				\begin{align*}
					\diff{s} &= s_{1 - 2} = \sqrt{s_1^2 + s_2^2} &\cint = \diff{\bar{x}} \pm t^*\frac{\diff{s}}{\sqrt{n}}
				\end{align*}
	\chapter{Significance Tests}
		A \textbf{significance test} is a procedure that uses observed data to test between two claims, often made regarding parameters, about hypotheses.
		\callout{17}{In order for a significance test to be performed, randomness, independence, and Normality must be verified.}
		The \textbf{null hypothesis} ($\pmb{H_0}$) claims that the parameter is equal to a \textbf{null value}, what it was previously assumed to be, denoted by a subscript $0$ on the parameter. It is often a statement of no change or difference.
		\[H_0:\rm{parameter} = \text{null value}\]
		The claim that is attempting to be supported is the \textbf{alternative hypothesis} ($\pmb{H_a}$). It can either be \textbf{one-sided}, claiming that the parameter is greater or less than the null value, or \textbf{two-sided}, claiming simply that the parameter is not equal to the null value.
		\[H_a:\rm{parameter} \gtrless \text{null value} \lor \rm{parameter} \ne \text{null value}\]
		A test's $\pmb{P}$\textbf{-value} is the probability of \emph{significant} evidence being found that supports $H_a$ that is at least as strong as that observed assuming that $H_0$ is true.
		\[P\text{-value} = P(\text{statistic supports } H_a \mid H_0)\] \\
		The smaller the $P$-value, the lower the chances of receiving evidence of the alternative. A small $P$-value therefore supports the $H_a$. \\
		If the $P$-value is less than the \textbf{significance level} $\pmb{\alpha}$, $H_0$ can be rejected and it can be concluded that there is convincing evidence for $H_a$. If the $P$-value is greater than or equal to $\alpha$, $H_0$ cannot be rejected, and it can be concluded that there is not convincing evidence for $H_a$. \\
		The $P$-value is calculated using the \textbf{standardized test statistic}, which is the number of standard deviations from the null value of the parameter. \\
		For a null hypothesis to be significant is for a significance test to provide a $P$-value less than the significance level.
		\[\text{standardized test statistic} = \z{\text{statistic}}{\text{null parameter}}{\text{standard error of statistic from null parameter}} = \mathrm{s}\]
		The $P$-value is equal to the probability of $z$ satisfying the $H_a$ assuming that $H_0$ is true. It can therefore be calculated as such using a \emph{cumulative distribution function}, so long as Normality and independence are justified.
		\[\small
			P\text{-value} = \begin{cases}
				P(\text{parameter} \gtrless \text{null parameter}) = P(\mathrm{S \gtrless s}) & H_a:\text{parameter} \gtrless \text{null parameter} \\
				P(|\text{parameter}| < |\text{null parameter}|) = P(\mathrm{S < -|s|}) + P(\mathrm{S > |s|}) & H_a:\text{parameter} \ne \text{null parameter}
			\end{cases}
		\]
		\callout{17}{Conclusions should only ever be made regarding the rejection of $H_0$ and the convincing support of $H_a$. $H_0$ should never be supported and $H_a$ should never be rejected.}
		\callout{19}{
				When answering a question regarding a significance test, the following process can be followed:
				\paragraph{1. State} 
					State the hypotheses to be tested and the significance level and define any parameters.
				\paragraph{2. Plan}
					Identify the appropriate methods of inference and verify its conditions.
				\paragraph{3. Do}
					State the sample statistic(s), calculate the standardized test statistic(s), and calculate the $P$-value.
				\paragraph{4. Conclude}
					Make a conclusion regarding the hypotheses within the problem's context.
			}
		When performing significance tests, two types of errors may occur:
		\begin{itemize}
			\item
				A \textbf{Type \Roman{1} error} occurs when $H_0$ is rejected despite $H_a$ being false; the data provided convincing evidence for $H_a$ despite it being incorrect. 
			\item
				A \textbf{Type \Roman{2} error} occurs when $H_0$ is not rejected despite $H_a$ being true; the data did not provide convincing evidence for $H_a$ despite it being correct.
		\end{itemize}
		\begin{center}
			\begin{tabular}{cc}
			&\begin{tabular}{cc}$H_a$ is false&\hspace{1.6cm}$H_a$ is true\end{tabular} \\
			\begin{tabular}{r}$H_0$ is rejected\\$H_0$ is not rejected\end{tabular} & \begin{tabular}{|c|c|}\hline Type \Roman{1} error&Correct conclusion\\\hline Correct conclusion&Type \Roman{2} error\\\hline\end{tabular}
		\end{tabular}
		\end{center}
		The probability of a Type \Roman{1} error occurring is equal to $\alpha$. \\
		As $\alpha$ increases, the probability of a Type \Roman{1} error increases but that of a Type \Roman{2} error decreases. \\
		A confidence interval for $\hat{p}$ (using $s_{\hat{p}}$) can be used in tandem with a sample statistic to provide a set of plausible values for the true parameter, should the alternative hypothesis be convincingly supported. \\
		A two-sided test of of $H_0$ at significance level $\alpha$ usually provides the same conclusion as a confidence level of the complement of $\alpha$.
		\[[P(\mathrm{S \ne s}) < \alpha] \approxident [\text{null parameter} \in (\text{statistic} \pm \ME)]\]
		A test's \textbf{power} is the probability of convincing evidence being found that convincingly supports $H_a$ given a value for the parameter being tested. This is also is equal to the probability of avoiding a Type \Roman{2} error. 
		\[\mathrm{power} = 1 - P(\text{Type \Roman{2} Error})^C = P(\text{statistic convincingly supports } H_a \mid H_a \text{ is true})\]
		Power can be increased in three ways:
		\begin{enumerate}
			\item 
				Increasing the sample size
				\begin{itemize}
					\item 
						A large sample means that more data is collected and more information is given regarding the true population parameter. This also increases $n$, which decreases the standard error of the statistic, reducing the value of the standardized test statistic and therefore the $P$-value, making it more likely to fall below $\alpha$.
				\end{itemize}
			\item 
				Increasing the significance level
				\begin{itemize}
					\item 
						Increasing the significance level increases the probability of $H_0$ being rejected when $H_a$ is true, as the maximum $P$-value for $H_0$ to be rejected increases.
				\end{itemize}
			\item 
				Increasing the \textbf{effect size}, the minimum difference between the null parameter value and the alternative parameter value for the change to matter
				\begin{itemize}
					\item 
						Increasing the size of the difference that needs to be detected makes that difference more likely to be detected, as larger differences are easier to detect.
				\end{itemize}
		\end{enumerate}
		\section{Significance Tests about Proportions}
			\callout{17}{In order for a significance test of $H_0:p = p_0$ to be performed, it must be verified that the distribution of $\hat{p}$ is approximately Normal assuming $H_0$ and that the standard error can be calculated, so Large Counts and the 10\% condition (not for experiments) must be satisfied and interpreted for Normality and independence respectively.}
			To perform \textbf{1-proportion $\pmb{z}$ test}, a significance test about one proportion, $z$ must be calculated. \\
			\[z = \frac{\hat{p} - \mu_{\hat{p}}}{\sigma_{\hat{p}}} = \frac{\hat{p} - p_0}{\sqrt{\frac{p_0(1 - p_0)}{n}}}\]			
			\subsection*{Significance Tests about Differences in Proportions}
				A \textbf{2-proportion $\pmb{z}$ test} can be performed to compare the proportions for two populations is based on the difference between sample proportions.
				\begin{align*}
					H_0:p_1 - p_2 = p_0 && H_a: p_1 - p_2 \gtrless p_0 && H_a:p_1 - p_2 \ne p_0
				\end{align*}
				Typically, $p_0$ is 0, so these hypotheses can be rewritten.
				\begin{align*}
					H_0:p_1 = p_2 && H_a:p_1 \gtrless p_2 && H_a:p_1 \ne p_2
				\end{align*}
				A significance test first assumes that the null hypothesis $H_0:p_1 = p_2$ is true. This common value is referred to as $p$. \\
				The \textbf{combined sample proportion} is denoted $\hat{p}_C$ and is equal to the total successes divided by the total sample size, making it effectively a weighted average. It is the sample proportion that assumes that the parameter values are equal.
				\[\hat{p}_C = \frac{x_1 + x_2}{n_1 + n_2} = \frac{n_1\hat{p}_1 + n_2\hat{p}_2}{n_1 + n_2}\]
				The Large Counts condition must be met with $\hat{p}_C$.
				\[n_1\hat{p}_C, n_1(1 - \hat{p}_C), n_2\hat{p}_C, n_2(1 - \hat{p}_C) \ge 10\]
				\callout{17}{For a significance test to be run about a difference of proportions, the randomness, independence (10\%) (for each proportion), and Large Counts conditions must be met.}
				The \emph{standardized test statistic} is the $z$-score calculated using the difference in proportions and its standard error assuming the mean to be 0 ($H_0$ to be true).
				\[z = \z{\hat{p}_1 - \hat{p}_2}{\mu_{\hat{p}_1 - \hat{p}_2}}{s_{\hat{p}_1 - \hat{p}_2}} = \z{\hat{p}_1 - \hat{p}_2}{0}{\propsed{\hat{p}_C}{n_1}{\hat{p}_C}{n_2}} = \z{\hat{p}_1}{\hat{p}_2}{\propsee{\hat{p}_C}{n_1}{n_2}}\]
		\section{Significance Tests about Means}
			\subsection*{Significance Tests about Differences in Means}
			\callout{17}{To perform a significance test for a population mean, a \textbf{1-sample t test} randomness, independence (10\%), and Normality (\emph{CLT} or distribution) must be verified.}
			The \emph{standardized test statistic} for a significance test about a mean is $t$.
			\[t = \z{\bar{x}}{\mu_{\bar{x}}}{s_{\bar{x}}} = \z{\bar{x}}{\mu_0}{s_x/\sqrt{n}}\]
			The \emph{$t$-distribution} used to calculate the $P$-value uses \emph{degrees of freedom} 1 less than the sample size.
			\[\df = n - 1\]
			\callout{17}{Minute, practically unimportant changes in in $\mu_0$ can drastically shrink the $P$-value when the sample size is always large enough. A very large sample size results in the null hypothesis almost always being rejected. \textbf{$\pmb{P}$-hacking} takes advantage of this fact.}
			
	\chapter{Chi-Square Tests}
	\chapter{Slopes}
\end{document} 