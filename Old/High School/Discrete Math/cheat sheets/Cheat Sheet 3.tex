\documentclass[12pt, A4, twocolumn]{article}

% Packages
	% Basics
		\usepackage{amsmath}
		\usepackage{bm}
		\usepackage{cellspace}
		\usepackage{csquotes}
		\usepackage[hang,flushmargin]{footmisc}
		\usepackage[margin=0.75in]{geometry}
		\usepackage{hyperref}
		\usepackage[utf8]{inputenc}
		\usepackage{moresize}
		\usepackage{multirow}
	% Diagrams
		\usepackage{pgfplots}
		\usepackage{tikz}
		\usepackage{tikz-3dplot}
			\usetikzlibrary{arrows.meta, angles, calc, quotes}
	% Symbols
		\usepackage{amssymb} % Miscellaneous
% Configuration
	\title{Test 3 Cheat Sheet}
	\author{Arnav Patri}
% Macros
	% Notation
		% Operators
			\DeclareMathOperator{\Exp}{\mathbb{E}}
			\DeclareMathOperator{\divr}{div}
			\DeclareMathOperator{\Var}{\mathbb{V}}
		% Sets
			\newcommand{\N}{\mathbb{N}}
			\newcommand{\R}{\mathbb{R}}
			\newcommand{\Z}{\mathbb{Z}}
		% Other
			\DeclareMathOperator{\avg}{avg}
			\renewcommand{\mod}{\text{mod}}
			\DeclareMathOperator{\return}{\text{return}}
			\renewcommand{\th}{\text{th}}
			\DeclareMathOperator{\while}{\text{while}}
	% Utilities
		\newcommand{\callout}[2]{\begin{center}\fbox{\begin{minipage}{#1cm}#2\end{minipage}}\end{center}}
		\newcommand{\comment}[1]{}
		\newcommand{\supt}[2]{#1^{\text{#2}}}
		\newcommand{\subsectionb}[1]{\subsection*{#1}\addcontentsline{toc}{subsection}{#1}}

\begin{document}
	Arnav Patri
	\setcounter{section}{6}	
	\section{Discrete Probability}
		A \textbf{sample space \(\bm{S}\)} is a set of possible outcomes. An \textbf{event} is a subset of the sample space. An \textbf{event \(\bm{E}\)} is a subset of the sample space. \\
		If \(S\) is a finite and nonempty sample space of equally likely outcomes and \(E \subseteq S\), the \textit{probability} of \(E\) is given by
			\[P(E) = \frac{|E|}{|S|}\]
\		\setcounter{subsection}{1}
		\subsection{Probability Theory}
			The probability of the \textbf{complement} of \(E\), defined as \(\bar{E} = S - E\), is found as
				\[P(\bar{E}) = 1 - P(E)\]
			The \textbf{intersection} of 2 events \(E_1\) and \(E_2\), denoted by \(E_1 \cap E_2\), is the event that they both occur. \\
				Two events are \textbf{disjoint} if the probability of their intersection is 0. \\
			The \textbf{union} of 2 events \(E_1\) and \(E_2\), denoted \(E_1 \cup E_2\), is the event that exactly one occurs. It is found as
				\[P(E_1 \cup E_2) = P(E_1) + P(E_2) - P(E_1 \cap E_2)\]
			Probabilities \(P(s)\) can be assigned for each \(s \in S\) such that \(0 \le P(s) \le 1\) for all \(s \in S\) and \(\sum\limits_{s \in S}P(s) = 1\). \\
			The function \(P\) from the element \(s \in S\) to the probability \(P(s)\) is a \textbf{probability distribution}. \\
			The probability of an event \(E\) can be found as
				\[P(E) = \sum_{s \in E} P(s)\]
			If \(E_1, E_2, \ldots\) is a sequence of pairwise disjoint events, then
				\[P\left(\bigcup_i E_i\right) = \sum_i P(E_i)\]
			If \(E_1\) and \(E_2\) are events with \(P(E_2) > 0\), the \textbf{conditional probability} of \(E_1\) given \(E_2\) is denoted and found as
				\[P(E_1 | E_2) = \frac{P(E_1 \cap E_2)}{P(E_2)}\]
				Two events are \textbf{independent} if
				\[P(E_1 \cap E_2) = P(E_1) \times P(E_2)\]
				A set of events is \textbf{pairwise independent} if any given 2 events are independent. It is \textbf{mutually independent} if every event is independent from every combination of all other events. \\
			A \textbf{Bernoulli trial} is a trial with binary outcomes. The probability of success is \(p\). \\
			The \textbf{Binomial distribution} gives the probability of \(X\) successes after \(n\) independent Bernoulli trials with a constant probability of success \(p\).
				\[P(x) = \binom{n}{x}p^x(1 - p)^x \tag{binomial}\]
				(\(P(x)\) is shorthand for \(P(X = x)\)). \\
			A \textbf{random variable} is a function from a sample space to the set of real numbers; that is, a random variable assigns a real number to every possible outcome. \\
			The \textbf{distribution} of a random variable \(X\) on a sample space \(S\) is the set of all pairs \((x, P(x))\) for all \(x \in X(S)\).
		\subsection{Bayes' Theorem}
			\textbf{Bayes' theorem} states that if \(E\) and \(F\) are events with nonzero probabilities,
				\[P(F | E) = \frac{P(E | F)P(F)}{P(E | F)P(F) + P(E|\bar{F})P(\bar{F})}\]
		\subsection{Expected Value and Variance}
			The \textbf{expected value} of a random variable \(X\) in sample space \(S\) is
				\[
					\Exp(X) = \mu_X 
						= \sum_{s \in S}P(s)X(s)
						= \sum_{x \in X}xP(x)
				\]
				The \textbf{deviation} of \(x \in X\) is \(x - \Exp(X)\). \\
				The expectation operator \(\Exp\) is linear, meaning that for 2 random variables \(X\) and \(Y\) and 2 constants \(a\) and \(b\),
				\[\Exp(aX + bY) = a\Exp(X) + b\Exp(Y)\]
				It should also be noted that
				\[\Exp(aX + b) = a\Exp(X) + b\]
				as the expected value of a constant is simply its value. \\
			The \textbf{Geometric distribution} gives the probability of getting the first success on the \(\supt{x}{th}\) independent Bernoulli trial with fixed probability of success \(p\). \\
				\[P(X) = (1 - p)^{k - 1}p \tag{geometric}\]
			Two random variables \(X\) and \(Y\) are \textbf{independent} if
				\[P(x \cap y) = P(x) \times P(y)\]
			The \textbf{variance} of \(X\) is
				\begin{align*}
					\Var(X) = \sigma^2_X &= \sum_{s \in S}(X(s) - \Exp(X))^2P(s) \\
						&= \sum_{x \in X}(x - \Exp(x))^2P(x)
				\end{align*}
			\textbf{Chebyshev's inequality} states that for a random variable \(X\) and a real number \(r\),
				\[P(|x - \Exp(x)| \ge r) \le \frac{\Var(X)}{r^2}\]
	\setcounter{section}{0}
	\section{Logic and Proofs}
		
\end{document}
